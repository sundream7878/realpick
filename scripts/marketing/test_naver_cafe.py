#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
env_path = project_root / '.env.local'
if env_path.exists():
    load_dotenv(dotenv_path=env_path)

from modules.naver_cafe_crawler import NaverCafeCrawler
from datetime import datetime, timedelta

def test_naver_cafe_crawl():
    """ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ì¹´í˜ URL 3ê°œ
    cafes = [
        {"id": "no1sejong", "name": "ì„¸ì¢…ë§˜ì¹´í˜", "url": "https://cafe.naver.com/no1sejong"},
        {"id": "chengnamomlife", "name": "ë‹¬ì½¤í•œ ì²­ë¼ë§˜ìŠ¤", "url": "https://cafe.naver.com/chengnamomlife"},
        {"id": "2008bunsamo", "name": "ë¶„ë”° (ë¶„ë‹¹.íŒêµ.ìœ„ë¡€)", "url": "https://cafe.naver.com/2008bunsamo"}
    ]
    
    # ë‚ ì§œ ë²”ìœ„ (ìµœê·¼ 30ì¼ - í…ŒìŠ¤íŠ¸ìš©)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)  # 30ì¼ ì „
    
    # ë¦¬ì–¼í”½ í˜„ì¬ ìš´ì˜ í”„ë¡œê·¸ë¨ í‚¤ì›Œë“œ
    keywords = [
        "ë‚˜ëŠ”ì†”ë¡œ", "ë‚˜ì†”",
        "ìµœê°•ì•¼êµ¬", "ëª¬ìŠ¤í„°ì¦ˆ",
        "ë‚˜ì†”ì‚¬ê³„",
        "ëŒì‹±ê¸€ì¦ˆ", "ëŒì‹±",
        "í™˜ìŠ¹ì—°ì• ", "í™˜ì—°",
        "ì†”ë¡œì§€ì˜¥",
        "í‘ë°±ìš”ë¦¬ì‚¬", "ì•ˆì„±ì¬", "ë°±ì¢…ì›",
        "ê³¨ë•Œë…€", "ê³¨ ë•Œë¦¬ëŠ” ê·¸ë…€ë“¤"
    ]
    
    print(f"\nğŸ“Œ í…ŒìŠ¤íŠ¸ ì„¤ì •:")
    print(f"  ì¹´í˜ ìˆ˜: {len(cafes)}ê°œ")
    for cafe in cafes:
        print(f"    - {cafe['name']}: {cafe['url']}")
    print(f"  í‚¤ì›Œë“œ ìˆ˜: {len(keywords)}ê°œ")
    print(f"  í‚¤ì›Œë“œ: {', '.join(keywords[:5])}... (ì´ {len(keywords)}ê°œ)")
    print(f"  ì‹œì‘ ì‹œê°„: {start_date.strftime('%Y-%m-%d %H:%M')} (30ì¼ ì „)")
    print(f"  ì¢…ë£Œ ì‹œê°„: {end_date.strftime('%Y-%m-%d %H:%M')} (í˜„ì¬)")
    print(f"  ìµœëŒ€ í˜ì´ì§€: 2 (í…ŒìŠ¤íŠ¸ìš©)")
    
    print(f"\nğŸš€ ì „ë¬¸ê°€ ê²€ì¦ 3ë‹¨ê³„ ë‚ ì§œ ì¶”ì¶œ ì „ëµ:")
    print(f"  1ï¸âƒ£ API ìš°ì„ : ë„¤ì´ë²„ Article APIë¡œ ë‚ ì§œ/ë³¸ë¬¸ í™•ë³´")
    print(f"  2ï¸âƒ£ ìƒì„¸ í˜ì´ì§€: PC í‘œì¤€ URL + iframe + ë‹¤ì¤‘ ì…€ë ‰í„°")
    print(f"  3ï¸âƒ£ ì •ê·œì‹ Fallback: 9ê°€ì§€ ë‚ ì§œ íŒ¨í„´ (ì‹œê°„ ì •ë³´ í¬í•¨)")
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = NaverCafeCrawler(headless=False, visible=True)
    
    try:
        # 1. ë¸Œë¼ìš°ì € ì‹œì‘
        print(f"\nğŸš€ 1ë‹¨ê³„: ë¸Œë¼ìš°ì € ì‹œì‘...")
        if not crawler.start_browser():
            print("âŒ ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨")
            return
        
        # 2. ë¡œê·¸ì¸ ëŒ€ê¸°
        print(f"\nâ³ 2ë‹¨ê³„: ë„¤ì´ë²„ ë¡œê·¸ì¸ ëŒ€ê¸° ì¤‘...")
        print("   ë¸Œë¼ìš°ì €ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤. ë„¤ì´ë²„ì— ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
        print("   ë¡œê·¸ì¸ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤...")
        
        if not crawler.wait_for_login(timeout=300):
            print("âŒ ë¡œê·¸ì¸ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼")
            crawler.close()
            return
        
        print("âœ… ë¡œê·¸ì¸ ì™„ë£Œ!")
        
        # 3. URL ì •ê·œí™” í…ŒìŠ¤íŠ¸
        print(f"\nğŸ”§ 3ë‹¨ê³„: URL ì •ê·œí™” í…ŒìŠ¤íŠ¸...")
        test_urls = [
            "https://cafe.naver.com/f-e/cafes/123456/articles/789012",
            "https://cafe.naver.com/ca-fe/web/articles/123456/789012",
            "https://cafe.naver.com/ArticleRead.nhn?clubid=123456&articleid=789012"
        ]
        
        for url in test_urls:
            normalized = crawler.normalize_article_url(url)
            print(f"  ì›ë³¸: {url[:60]}...")
            print(f"  ì •ê·œí™”: {normalized}")
        
        # 4. ëª©ë¡ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (3ê°œ ì¹´í˜ ìˆœíšŒ)
        print(f"\nğŸ“‹ 4ë‹¨ê³„: ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (3ê°œ ì¹´í˜, 2í˜ì´ì§€)")
        print(f"   í‚¤ì›Œë“œ: {', '.join(keywords[:3])}... (ì´ {len(keywords)}ê°œ)")
        
        all_posts = []
        for cafe_idx, cafe in enumerate(cafes, 1):
            print(f"\nğŸ  ì¹´í˜ {cafe_idx}/{len(cafes)}: {cafe['name']}")
            print(f"   URL: {cafe['url']}")
            
            posts_list = crawler.crawl_article_list(
                cafe_url=cafe['url'],
                keywords=keywords,
                start_date=start_date,
                end_date=end_date,
                exclude_boards=["ë¨¹ê±°ë¦¬", "ë§›ì§‘", "í”„ë¦¬ë§ˆì¼“"],
                max_pages=2
            )
            
            all_posts.extend(posts_list)
            print(f"   âœ… {cafe['name']}: {len(posts_list)}ê°œ ìˆ˜ì§‘")
        
        print(f"\nâœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_posts)}ê°œ ê²Œì‹œê¸€")
        
        if all_posts:
            print(f"\nğŸ“ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ìƒ˜í”Œ (ìµœëŒ€ 3ê°œ):")
            for i, post in enumerate(all_posts[:3], 1):
                print(f"\n{'='*80}")
                print(f"[{i}] ì œëª©: {post.get('title', 'N/A')}")
                print(f"{'='*80}")
                print(f"ì¹´í˜: {post.get('cafe_url', 'N/A').split('/')[-1]}")
                print(f"ë‚ ì§œ: {post.get('date', 'N/A')}")
                print(f"ì¡°íšŒìˆ˜: {post.get('viewCount', 0)}")
                print(f"ëŒ“ê¸€ìˆ˜: {post.get('commentCount', len(post.get('comments', [])))}")
                print(f"ì‘ì„±ì: {post.get('nickname', 'N/A')}")
                print(f"ê²Œì‹œíŒ: {post.get('board_name', 'N/A')}")
                print(f"\nğŸ“„ ë³¸ë¬¸ ë‚´ìš©:")
                print(f"{'-'*80}")
                content = post.get('content', '')
                if content:
                    # ë³¸ë¬¸ ë‚´ìš© ì¶œë ¥ (ìµœëŒ€ 500ìê¹Œì§€)
                    if len(content) > 500:
                        print(content[:500] + f"\n... (ì´ {len(content)}ì)")
                    else:
                        print(content)
                else:
                    print("(ë³¸ë¬¸ ì—†ìŒ)")
                print(f"{'-'*80}")
        
        # 5. ìƒì„¸ ìˆ˜ì§‘ ì—¬ë¶€ í™•ì¸
        has_content = any(post.get('content') and len(post.get('content', '')) > 100 for post in all_posts)
        if has_content:
            print(f"\nâœ… ìƒì„¸ ì •ë³´(ë³¸ë¬¸) ìˆ˜ì§‘ ì™„ë£Œ: ì¼ë¶€ ê²Œì‹œê¸€ì— ë³¸ë¬¸ í¬í•¨")
        else:
            print(f"\nâš ï¸ ìƒì„¸ ì •ë³´ ì—†ìŒ: ë‚ ì§œë§Œ ìˆ˜ì§‘ë¨")
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"\nğŸ“Š ìˆ˜ì§‘ ìš”ì•½:")
        print(f"   - ì¹´í˜ ìˆ˜: {len(cafes)}ê°œ")
        print(f"   - í‚¤ì›Œë“œ ìˆ˜: {len(keywords)}ê°œ")
        print(f"   - ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(all_posts)}ê°œ")
        if all_posts:
            # ì¹´í˜ë³„ í†µê³„
            cafe_stats = {}
            for post in all_posts:
                cafe_id = post.get('cafe_url', '').split('/')[-1]
                cafe_stats[cafe_id] = cafe_stats.get(cafe_id, 0) + 1
            
            print(f"   - ì¹´í˜ë³„:")
            for cafe_id, count in cafe_stats.items():
                cafe_name = next((c['name'] for c in cafes if cafe_id in c['url']), cafe_id)
                print(f"     â€¢ {cafe_name}: {count}ê°œ")
        
        print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. ì „ì²´ í¬ë¡¤ë§ì„ ì›í•˜ë©´ max_pagesë¥¼ ëŠ˜ë¦¬ì„¸ìš”")
        print(f"   2. ë” ë§ì€ ì¹´í˜ë¥¼ ì¶”ê°€í•˜ë ¤ë©´ cafes ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”")
        print(f"   3. APIë¥¼ í†µí•´ í¬ë¡¤ë§í•˜ë ¤ë©´ /api/admin/marketer/naver-cafe/crawl ì‚¬ìš©")
        
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        print(f"\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘...")
        crawler.close()
        print(f"âœ… ì™„ë£Œ")

if __name__ == "__main__":
    test_naver_cafe_crawl()
