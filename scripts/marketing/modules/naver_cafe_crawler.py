"""
ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ëŸ¬ (Selenium ê¸°ë°˜)
PC/SPA í˜¼ì¬ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ê²Œì‹œê¸€ ìˆ˜ì§‘
"""

import sys
import time
import re
import json
import random
import os
import pickle
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse, parse_qs
from pathlib import Path

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    import undetected_chromedriver as uc
    HAS_SELENIUM = True
except ImportError as e:
    HAS_SELENIUM = False
    print(f"[Naver Cafe Crawler] âš ï¸ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print(f"[Naver Cafe Crawler] ì„¤ì¹˜ ë°©ë²•:", file=sys.stderr)
    print(f"[Naver Cafe Crawler]   pip install selenium undetected-chromedriver", file=sys.stderr)
    print(f"[Naver Cafe Crawler] ë˜ëŠ”:", file=sys.stderr)
    print(f"[Naver Cafe Crawler]   pip install -r requirements.txt", file=sys.stderr)
    print(f"[Naver Cafe Crawler] ì˜¤ë¥˜ ìƒì„¸: {e}", file=sys.stderr)

import requests
from bs4 import BeautifulSoup


class NaverCafeCrawler:
    """ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ëŸ¬ - Selenium + API í•˜ì´ë¸Œë¦¬ë“œ"""
    
    def __init__(self, headless: bool = False, visible: bool = True):
        """
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (ë¹„ê¶Œì¥)
            visible: ë¸Œë¼ìš°ì € í‘œì‹œ (ê¶Œì¥)
        """
        self.driver = None
        self.headless = headless
        self.visible = visible
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
    
    def start_browser(self) -> bool:
        """ë¸Œë¼ìš°ì € ì‹œì‘ (ìˆ˜ë™ ë¡œê·¸ì¸ ëŒ€ê¸°)"""
        if not HAS_SELENIUM:
            print("[Naver Cafe Crawler] âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
            return False
        
        try:
            options = uc.ChromeOptions()
            if not self.headless and self.visible:
                options.add_argument('--start-maximized')
            else:
                options.add_argument('--headless')
            
            options.add_argument('--disable-gpu')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            
            self.driver = uc.Chrome(options=options, version_main=None)
            self.driver.implicitly_wait(10)
            
            print("[Naver Cafe Crawler] âœ… ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ. ë„¤ì´ë²„ ë¡œê·¸ì¸ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.", file=sys.stderr)
            return True
        except Exception as e:
            print(f"[Naver Cafe Crawler] âŒ ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨: {e}", file=sys.stderr)
            return False
    
    def check_login_status(self) -> bool:
        """í˜„ì¬ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸"""
        if not self.driver:
            return False
        
        try:
            # í˜„ì¬ í˜ì´ì§€ ì €ì¥
            current_url = self.driver.current_url
            
            # ë„¤ì´ë²„ ë©”ì¸ìœ¼ë¡œ ì´ë™í•˜ì—¬ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
            self.driver.get("https://www.naver.com")
            time.sleep(2)
            
            # ë¡œê·¸ì¸ ë²„íŠ¼ì´ ìˆìœ¼ë©´ ë¡œê·¸ì•„ì›ƒ ìƒíƒœ
            try:
                login_buttons = self.driver.find_elements(By.CSS_SELECTOR, "a.link_login, a[href*='nidlogin']")
                if login_buttons and any(btn.is_displayed() for btn in login_buttons):
                    print("[Naver Cafe Crawler] âš ï¸ ë¡œê·¸ì¸ ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", file=sys.stderr)
                    return False
            except:
                pass
            
            # ì›ë˜ í˜ì´ì§€ë¡œ ë³µê·€
            if current_url and 'naver.com' in current_url:
                self.driver.get(current_url)
                time.sleep(1)
            
            return True
        except Exception as e:
            print(f"[Naver Cafe Crawler] ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}", file=sys.stderr)
            return True  # ì˜¤ë¥˜ ì‹œ ì¼ë‹¨ ì§„í–‰
    
    def wait_for_login(self, timeout: int = 300, save_cookies: bool = True) -> bool:
        """ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸° (ìˆ˜ë™) - ì™„ë£Œ í›„ ì¿ í‚¤ ìë™ ì €ì¥"""
        if not self.driver:
            return False
        
        try:
            self.driver.get("https://nid.naver.com/nidlogin.login")
            print(f"[Naver Cafe Crawler] â³ ë¡œê·¸ì¸ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”. (ìµœëŒ€ {timeout}ì´ˆ ëŒ€ê¸°)", file=sys.stderr)
            
            # ë¡œê·¸ì¸ ì™„ë£Œ í™•ì¸ (ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™í–ˆëŠ”ì§€ í™•ì¸)
            WebDriverWait(self.driver, timeout).until(
                lambda d: 'naver.com' in d.current_url and 'nidlogin' not in d.current_url
            )
            
            print("[Naver Cafe Crawler] âœ… ë¡œê·¸ì¸ ì™„ë£Œ í™•ì¸", file=sys.stderr)
            
            # ë¡œê·¸ì¸ ì„±ê³µ ì‹œ ì¿ í‚¤ ìë™ ì €ì¥ (ë‹¤ìŒì— ì¬ì‚¬ìš©)
            if save_cookies:
                time.sleep(2)  # ì¿ í‚¤ ì„¤ì • ëŒ€ê¸°
                self.save_login_cookies()
                print("[Naver Cafe Crawler] ğŸ’¡ ì¿ í‚¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.", file=sys.stderr)
            
            return True
        except TimeoutException:
            print("[Naver Cafe Crawler] âš ï¸ ë¡œê·¸ì¸ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼", file=sys.stderr)
            return False
        except Exception as e:
            print(f"[Naver Cafe Crawler] âŒ ë¡œê·¸ì¸ í™•ì¸ ì˜¤ë¥˜: {e}", file=sys.stderr)
            return False
    
    def normalize_article_url(self, url: str) -> Optional[str]:
        """
        [2026 ì „ë¬¸ê°€ ê²€ì¦] ëª¨ë“  URLì„ PC í‘œì¤€ìœ¼ë¡œ ì •ê·œí™”
        
        ì§€ì› í˜•ì‹:
        - 2026ë…„ SPA: https://cafe.naver.com/f-e/cafes/123456/articles/789012
        - êµ¬ì‹ SPA: https://cafe.naver.com/cafes/123456/articles/789012
        - ëª¨ë°”ì¼: https://cafe.naver.com/ca-fe/web/articles/123456/789012
        - PC í‘œì¤€: https://cafe.naver.com/ArticleRead.nhn?clubid=123456&articleid=789012
        - ì´ì¤‘ ì¸ì½”ë”©: https://cafe.naver.com/cafeid?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D...
        
        ë°˜í™˜: PC í‘œì¤€ URL (iframe/ë³¸ë¬¸ ì¶”ì¶œì´ ì•ˆì •ì )
        """
        if not url or 'cafe.naver.com' not in url:
            return None
        
        # ì´ì¤‘ ì¸ì½”ë”© ì²˜ë¦¬ (iframe_url_utf8 íŒŒë¼ë¯¸í„°)
        if 'iframe_url_utf8=' in url:
            try:
                from urllib.parse import unquote
                # iframe_url_utf8 íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                match = re.search(r'iframe_url_utf8=([^&]+)', url)
                if match:
                    encoded_path = match.group(1)
                    # ì´ì¤‘ ë””ì½”ë”©
                    decoded_path = unquote(unquote(encoded_path))
                    # clubid, articleid ì¶”ì¶œ
                    clubid_match = re.search(r'clubid[=:](\d+)', decoded_path)
                    articleid_match = re.search(r'articleid[=:](\d+)', decoded_path)
                    if clubid_match and articleid_match:
                        clubid = clubid_match.group(1)
                        articleid = articleid_match.group(1)
                        return f"https://cafe.naver.com/ArticleRead.nhn?clubid={clubid}&articleid={articleid}"
            except Exception as e:
                print(f"[Naver Cafe Crawler] ì´ì¤‘ ì¸ì½”ë”© URL íŒŒì‹± ì˜¤ë¥˜: {e}", file=sys.stderr)
        
        clubid = None
        articleid = None
        
        # 2026ë…„ ìµœì‹  SPA í˜•ì‹ (/f-e/ í¬í•¨)
        match = re.search(r'/f-e/cafes/(\d+)/articles/(\d+)', url)
        if match:
            clubid, articleid = match.groups()
        
        # êµ¬ì‹ SPA í˜•ì‹
        if not clubid:
            match = re.search(r'/cafes/(\d+)/articles/(\d+)', url)
            if match:
                clubid, articleid = match.groups()
        
        # ëª¨ë°”ì¼ í˜•ì‹
        if not clubid:
            match = re.search(r'/ca-fe/web/articles/(\d+)/(\d+)', url)
            if match:
                clubid, articleid = match.groups()
        
        # PC í‘œì¤€ í˜•ì‹ (ì´ë¯¸ PC í‘œì¤€ì´ë©´ ê·¸ëŒ€ë¡œ)
        if '/ArticleRead.nhn' in url:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            clubid = params.get('clubid', [None])[0]
            articleid = params.get('articleid', [None])[0]
        
        # URL íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not clubid:
            parsed = urlparse(url)
            if 'clubid' in parsed.query and 'articleid' in parsed.query:
                params = parse_qs(parsed.query)
                clubid = params.get('clubid', [None])[0]
                articleid = params.get('articleid', [None])[0]
        
        # PC í‘œì¤€ URLë¡œ ë°˜í™˜ (ì „ë¬¸ê°€ ê²€ì¦: iframe/ë³¸ë¬¸ ì¶”ì¶œ ì•ˆì •)
        if clubid and articleid:
            return f"https://cafe.naver.com/ArticleRead.nhn?clubid={clubid}&articleid={articleid}"
        
        return url  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def save_debug_screenshot(self, filename_prefix: str):
        """ë””ë²„ê¹…ìš© ìŠ¤í¬ë¦°ìƒ· ì €ì¥"""
        if not self.driver:
            return
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            screenshot_path = f"debug_{filename_prefix}_{timestamp}.png"
            self.driver.save_screenshot(screenshot_path)
            print(f"[Naver Cafe Crawler] ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}", file=sys.stderr)
        except Exception as e:
            print(f"[Naver Cafe Crawler] ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}", file=sys.stderr)
    
    def get_article_info_from_api(self, clubid: str, articleid: str) -> Optional[dict]:
        """
        [ì „ë¬¸ê°€ ê²€ì¦] Naver Article APIë¡œ ë‚ ì§œ/ë³¸ë¬¸ í™•ë³´
        
        ëª©ë¡ DOMì— ë‚ ì§œê°€ ì—†ëŠ” ë¬¸ì œë¥¼ APIë¡œ í•´ê²°
        """
        try:
            api_url = f"https://apis.naver.com/cafe-web/cafe-article/v1/articles/{articleid}?useCafeId=false&buid={clubid}"
            
            # ë¡œê·¸ì¸ ì¿ í‚¤ ì‚¬ìš©
            cookies = {}
            if self.driver:
                for cookie in self.driver.get_cookies():
                    cookies[cookie['name']] = cookie['value']
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Referer': f'https://cafe.naver.com/ArticleRead.nhn?clubid={clubid}&articleid={articleid}'
            }
            
            import requests
            response = requests.get(api_url, headers=headers, cookies=cookies, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                result = data.get('result', {})
                article = result.get('article', {})
                
                if not article:
                    print(f"[Naver Cafe Crawler] âš ï¸ API ì‘ë‹µì— article ì—†ìŒ. data keys: {list(data.keys())}", file=sys.stderr)
                    return None
                
                # ë‚ ì§œ ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œ ì‹œë„)
                write_date = article.get('writeDate') or article.get('writeDateTimestamp') or article.get('createdAt')
                
                if not write_date:
                    print(f"[Naver Cafe Crawler] âš ï¸ API articleì— ë‚ ì§œ ì—†ìŒ. article keys: {list(article.keys())}", file=sys.stderr)
                    return None
                
                print(f"[Naver Cafe Crawler] âœ… API ì„±ê³µ: ë‚ ì§œ={write_date}", file=sys.stderr)
                return {
                    'date': write_date,
                    'content': article.get('content', ''),
                    'subject': article.get('subject', ''),
                    'nickname': article.get('writerNickname', ''),
                    'member_id': article.get('writerId', '')
                }
            
            print(f"[Naver Cafe Crawler] âš ï¸ API HTTP ì˜¤ë¥˜: status={response.status_code}", file=sys.stderr)
            return None
            
        except Exception as e:
            print(f"[Naver Cafe Crawler] âš ï¸ API í˜¸ì¶œ ì˜¤ë¥˜: {e}", file=sys.stderr)
            return None
    
    def save_page_source(self, filename_prefix: str):
        """ë””ë²„ê¹…ìš© HTML ì†ŒìŠ¤ ì €ì¥"""
        if not self.driver:
            return
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_path = f"debug_{filename_prefix}_{timestamp}.html"
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(self.driver.page_source)
            print(f"[Naver Cafe Crawler] ğŸ’¾ HTML ì €ì¥: {html_path}", file=sys.stderr)
        except Exception as e:
            print(f"[Naver Cafe Crawler] HTML ì €ì¥ ì‹¤íŒ¨: {e}", file=sys.stderr)
    
    def save_login_cookies(self, filepath: str = "naver_cookies.pkl"):
        """ë¡œê·¸ì¸ ì¿ í‚¤ ì €ì¥"""
        if not self.driver:
            return
        
        try:
            cookies = self.driver.get_cookies()
            with open(filepath, 'wb') as f:
                pickle.dump(cookies, f)
            print(f"[Naver Cafe Crawler] ğŸª ì¿ í‚¤ ì €ì¥: {filepath}", file=sys.stderr)
        except Exception as e:
            print(f"[Naver Cafe Crawler] ì¿ í‚¤ ì €ì¥ ì‹¤íŒ¨: {e}", file=sys.stderr)
    
    def load_login_cookies(self, filepath: str = "naver_cookies.pkl") -> bool:
        """ì €ì¥ëœ ì¿ í‚¤ ë¡œë“œ (ë¡œê·¸ì¸ ì„¸ì…˜ ì¬ì‚¬ìš©)"""
        if not self.driver:
            return False
        
        try:
            if not os.path.exists(filepath):
                print(f"[Naver Cafe Crawler] ì €ì¥ëœ ì¿ í‚¤ ì—†ìŒ: {filepath}", file=sys.stderr)
                return False
            
            # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸ (ì¿ í‚¤ ë„ë©”ì¸ ì„¤ì •)
            self.driver.get("https://www.naver.com")
            time.sleep(2)
            
            with open(filepath, 'rb') as f:
                cookies = pickle.load(f)
            
            for cookie in cookies:
                try:
                    self.driver.add_cookie(cookie)
                except Exception as e:
                    # ì¼ë¶€ ì¿ í‚¤ëŠ” ì¶”ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ (ë„ë©”ì¸ ë¶ˆì¼ì¹˜ ë“±)
                    pass
            
            print(f"[Naver Cafe Crawler] ğŸª ì¿ í‚¤ ë¡œë“œ ì™„ë£Œ: {len(cookies)}ê°œ", file=sys.stderr)
            
            # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
            self.driver.get("https://www.naver.com")
            time.sleep(2)
            
            # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ (ë¡œê·¸ì¸ ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ë¡œê·¸ì¸ ìƒíƒœ)
            try:
                login_button = self.driver.find_elements(By.CSS_SELECTOR, "a.link_login")
                if login_button:
                    print(f"[Naver Cafe Crawler] âš ï¸ ì¿ í‚¤ ë¡œë“œí–ˆì§€ë§Œ ë¡œê·¸ì¸ ìƒíƒœ ì•„ë‹˜", file=sys.stderr)
                    return False
                else:
                    print(f"[Naver Cafe Crawler] âœ… ì¿ í‚¤ë¡œ ë¡œê·¸ì¸ ìƒíƒœ ë³µì› ì„±ê³µ", file=sys.stderr)
                    return True
            except:
                return True
            
        except Exception as e:
            print(f"[Naver Cafe Crawler] ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
            return False
    
    def switch_to_iframe_if_needed(self) -> bool:
        """iframe ì „í™˜ (PC í‘œì¤€ í˜ì´ì§€) - ëª…ì‹œì  ëŒ€ê¸° ê°•í™”"""
        if not self.driver:
            return False
        
        try:
            # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „í™˜
            self.driver.switch_to.default_content()
            
            current_url = self.driver.current_url
            # SPA/ëª¨ë°”ì¼ URLì´ë©´ iframe ì—†ìŒ
            if '/f-e/' in current_url or '/ca-fe/' in current_url:
                print("[Naver Cafe Crawler] SPA í˜ì´ì§€ ê°ì§€ (iframe ì—†ìŒ)", file=sys.stderr)
                return True
            
            # iframe ì°¾ê¸° ë° ì „í™˜ (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
            try:
                iframe = WebDriverWait(self.driver, 5).until(
                    EC.frame_to_be_available_and_switch_to_it((By.ID, "cafe_main"))
                )
                print("[Naver Cafe Crawler] âœ… cafe_main í”„ë ˆì„ ì „í™˜ ì„±ê³µ", file=sys.stderr)
                
                # iframe ë‚´ë¶€ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° (1ì´ˆë¡œ ë‹¨ì¶•)
                time.sleep(1)
                return True
            except TimeoutException:
                # iframeì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì§„í–‰
                print("[Naver Cafe Crawler] âš ï¸ cafe_main iframe ì—†ìŒ (ê³„ì† ì§„í–‰)", file=sys.stderr)
                return True
        except Exception as e:
            print(f"[Naver Cafe Crawler] âŒ iframe ì „í™˜ ì˜¤ë¥˜: {e}", file=sys.stderr)
            # ì‹¤íŒ¨ ì‹œ ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            self.save_debug_screenshot("iframe_switch_failed")
            return False
    
    def extract_member_id_from_api(self, clubid: str, articleid: str) -> Optional[Dict[str, str]]:
        """
        API ìš°íšŒë¡œ ì‘ì„±ì ê³ ìœ í‚¤(member_id) ë° ë‹‰ë„¤ì„ ì¶”ì¶œ
        
        Returns:
            {'member_id': '...', 'nickname': '...'} ë˜ëŠ” None
        """
        try:
            api_url = f"https://apis.naver.com/cafe-web/cafe-article/v1/articles/{articleid}?useCafeId=false&buid={clubid}"
            response = self.session.get(api_url, timeout=10)
            
            if response.status_code != 200:
                return None
            
            data = response.json()
            
            # JSON êµ¬ì¡° íƒìƒ‰
            writer = None
            if 'result' in data and 'article' in data['result']:
                writer = data['result']['article'].get('writer')
            elif 'article' in data:
                writer = data['article'].get('writer')
            elif 'writer' in data:
                writer = data['writer']
            
            if not writer:
                return None
            
            # member_id ì¶”ì¶œ (ì—¬ëŸ¬ í›„ë³´)
            member_id = (
                writer.get('id') or
                writer.get('memberKey') or
                writer.get('memberId') or
                writer.get('userKey') or
                writer.get('userId') or
                None
            )
            
            # nickname ì¶”ì¶œ
            nickname = (
                writer.get('nickname') or
                writer.get('nickName') or
                writer.get('displayName') or
                writer.get('name') or
                'Unknown'
            )
            
            if member_id:
                return {'member_id': str(member_id), 'nickname': nickname}
            
            return None
        except Exception as e:
            print(f"[Naver Cafe Crawler] API member_id ì¶”ì¶œ ì˜¤ë¥˜: {e}", file=sys.stderr)
            return None
    
    def _extract_clubid_from_cafe(self, cafe_url: str) -> Optional[str]:
        """
        ì¹´í˜ ë©”ì¸ í˜ì´ì§€ì—ì„œ ìˆ«ì clubid ì¶”ì¶œ
        
        Args:
            cafe_url: ì¹´í˜ URL (ì˜ˆ: https://cafe.naver.com/imsanbu)
        
        Returns:
            ìˆ«ì clubid (ì˜ˆ: "10050146") ë˜ëŠ” None
        """
        if not self.driver:
            return None
        
        try:
            # ì¹´í˜ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            print(f"[Naver Cafe Crawler] ì¹´í˜ í˜ì´ì§€ ì ‘ì†: {cafe_url}", file=sys.stderr)
            self.driver.get(cafe_url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ìµœì†Œí™”)
            time.sleep(2)
            
            # JavaScript ì™„ë£Œ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
            try:
                from selenium.webdriver.support.ui import WebDriverWait
                WebDriverWait(self.driver, 5).until(
                    lambda d: d.execute_script("return document.readyState") == "complete"
                )
                print(f"[Naver Cafe Crawler] âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ", file=sys.stderr)
            except:
                print(f"[Naver Cafe Crawler] âš ï¸ JavaScript ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ (ê³„ì† ì§„í–‰)", file=sys.stderr)
            
            # ì¶”ê°€ ì•ˆì •í™” ëŒ€ê¸° (1ì´ˆë¡œ ë‹¨ì¶•)
            time.sleep(1)
            
            # í˜„ì¬ URL í™•ì¸ (ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²´í¬)
            current_url = self.driver.current_url
            print(f"[Naver Cafe Crawler] í˜„ì¬ URL: {current_url}", file=sys.stderr)
            
            # í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ clubid ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            page_source = self.driver.page_source
            print(f"[Naver Cafe Crawler] í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´: {len(page_source)}ì", file=sys.stderr)
            
            # íŒ¨í„´ 1: g_sClubId = "12345678"
            match = re.search(r'g_sClubId\s*=\s*["\'](\d+)["\']', page_source)
            if match:
                clubid = match.group(1)
                print(f"[Naver Cafe Crawler] âœ… clubid ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´1 g_sClubId): {clubid}", file=sys.stderr)
                return clubid
            
            # íŒ¨í„´ 2: clubid=12345678 (URL íŒŒë¼ë¯¸í„°)
            match = re.search(r'[?&]clubid=(\d+)', current_url, re.IGNORECASE)
            if match:
                clubid = match.group(1)
                print(f"[Naver Cafe Crawler] âœ… clubid ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´2 URL): {clubid}", file=sys.stderr)
                return clubid
            
            # íŒ¨í„´ 3: clubid=12345678 (í˜ì´ì§€ ì†ŒìŠ¤)
            match = re.search(r'clubid[=:](\d+)', page_source, re.IGNORECASE)
            if match:
                clubid = match.group(1)
                print(f"[Naver Cafe Crawler] âœ… clubid ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´3 clubid=): {clubid}", file=sys.stderr)
                return clubid
            
            # íŒ¨í„´ 4: "clubId":"12345678"
            match = re.search(r'"clubId"\s*:\s*"(\d+)"', page_source)
            if match:
                clubid = match.group(1)
                print(f"[Naver Cafe Crawler] âœ… clubid ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´4 JSON clubId): {clubid}", file=sys.stderr)
                return clubid
            
            # íŒ¨í„´ 5: "clubId":12345678 (ë”°ì˜´í‘œ ì—†ëŠ” ìˆ«ì)
            match = re.search(r'"clubId"\s*:\s*(\d+)', page_source)
            if match:
                clubid = match.group(1)
                print(f"[Naver Cafe Crawler] âœ… clubid ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´5 JSON clubId ìˆ«ì): {clubid}", file=sys.stderr)
                return clubid
            
            # íŒ¨í„´ 6: data-clubid="12345678"
            match = re.search(r'data-clubid\s*=\s*["\'](\d+)["\']', page_source, re.IGNORECASE)
            if match:
                clubid = match.group(1)
                print(f"[Naver Cafe Crawler] âœ… clubid ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´6 data-clubid): {clubid}", file=sys.stderr)
                return clubid
            
            # íŒ¨í„´ 7: cafe.naver.com/cafeid â†’ APIë¡œ clubid ì¡°íšŒ ì‹œë„
            cafe_name = re.search(r'cafe\.naver\.com/([^/?]+)', cafe_url)
            if cafe_name:
                cafe_name_str = cafe_name.group(1)
                # í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ í•´ë‹¹ ì¹´í˜ëª…ê³¼ í•¨ê»˜ ë‚˜ì˜¤ëŠ” ìˆ«ì ID ì°¾ê¸°
                pattern = rf'{cafe_name_str}[^0-9]*?(\d{{7,10}})'
                match = re.search(pattern, page_source)
                if match:
                    clubid = match.group(1)
                    print(f"[Naver Cafe Crawler] âœ… clubid ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´7 ì¹´í˜ëª… ê·¼ì²˜): {clubid}", file=sys.stderr)
                    return clubid
            
            # ì‹¤íŒ¨ ì‹œ ë””ë²„ê¹… ì •ë³´
            print(f"[Naver Cafe Crawler] âŒ clubid ì¶”ì¶œ ì‹¤íŒ¨", file=sys.stderr)
            print(f"[Naver Cafe Crawler] í˜ì´ì§€ ì†ŒìŠ¤ ìƒ˜í”Œ (ì²˜ìŒ 1000ì):", file=sys.stderr)
            print(f"{page_source[:1000]}", file=sys.stderr)
            print(f"\n[Naver Cafe Crawler] í˜ì´ì§€ ì†ŒìŠ¤ ìƒ˜í”Œ (ë§ˆì§€ë§‰ 500ì):", file=sys.stderr)
            print(f"{page_source[-500:]}", file=sys.stderr)
            
            # ê°€ì… í•„ìš” ë©”ì‹œì§€ ì²´í¬
            if 'ê°€ì…í•˜ê¸°' in page_source or 'ì¹´í˜ ê°€ì…' in page_source or 'ë©¤ë²„ ê°€ì…' in page_source:
                print(f"[Naver Cafe Crawler] âš ï¸ ì¹´í˜ ê°€ì…ì´ í•„ìš”í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.", file=sys.stderr)
            
            if 'ë¹„ê³µê°œ' in page_source or 'ì ‘ê·¼ ê¶Œí•œ' in page_source:
                print(f"[Naver Cafe Crawler] âš ï¸ ë¹„ê³µê°œ ì¹´í˜ì´ê±°ë‚˜ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
            
            # ë¡œê·¸ì¸ ìƒíƒœ ì²´í¬
            if 'ë¡œê·¸ì¸' in page_source and 'login' in page_source.lower():
                print(f"[Naver Cafe Crawler] âš ï¸ ë¡œê·¸ì¸ì´ í•„ìš”í•˜ê±°ë‚˜ ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", file=sys.stderr)
            
            # ë””ë²„ê¹…ìš© HTML ì €ì¥
            try:
                cafe_name = re.search(r'cafe\.naver\.com/([^/?]+)', cafe_url)
                if cafe_name:
                    cafe_name_str = cafe_name.group(1)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    debug_file = f"debug_cafe_{cafe_name_str}_{timestamp}.html"
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(page_source)
                    print(f"[Naver Cafe Crawler] ğŸ’¾ ë””ë²„ê¹…ìš© HTML ì €ì¥: {debug_file}", file=sys.stderr)
            except:
                pass
            
            return None
            
        except Exception as e:
            print(f"[Naver Cafe Crawler] clubid ì¶”ì¶œ ì˜¤ë¥˜: {e}", file=sys.stderr)
            return None
    
    def parse_date_from_text(self, text: str) -> Optional[datetime]:
        """ë‚ ì§œ í…ìŠ¤íŠ¸ íŒŒì‹± (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)"""
        if not text:
            return None
        
        now = datetime.now()
        
        # [ìµœìš°ì„ ] ì‹œê°„ ì •ë³´ í¬í•¨ í˜•ì‹
        # YYYY.MM.DD. HH:MM (ë„¤ì´ë²„ ì¹´í˜ í‘œì¤€)
        match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})\.?\s+(\d{1,2}):(\d{2})', text)
        if match:
            year, month, day, hour, minute = map(int, match.groups())
            try:
                return datetime(year, month, day, hour, minute)
            except:
                pass
        
        # YYYY.MM.DD HH:MM (ê³µë°± í¬í•¨)
        match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})\s+(\d{1,2}):(\d{2})', text)
        if match:
            year, month, day, hour, minute = map(int, match.groups())
            try:
                return datetime(year, month, day, hour, minute)
            except:
                pass
        
        # YYYY-MM-DD HH:MM
        match = re.search(r'(\d{4})-(\d{2})-(\d{2})\s+(\d{1,2}):(\d{2})', text)
        if match:
            year, month, day, hour, minute = map(int, match.groups())
            try:
                return datetime(year, month, day, hour, minute)
            except:
                pass
        
        # HH:MM í˜•ì‹ë§Œ ìˆìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¡œ
        match = re.search(r'^(\d{2}):(\d{2})$', text.strip())
        if match:
            hour, minute = map(int, match.groups())
            try:
                return datetime(now.year, now.month, now.day, hour, minute)
            except:
                return now
        
        # YYYY.MM.DD í˜•ì‹ (ì‹œê°„ ì—†ìŒ)
        match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})\.?', text)
        if match:
            year, month, day = map(int, match.groups())
            try:
                return datetime(year, month, day)
            except:
                pass
        
        # YYYY-MM-DD í˜•ì‹ (ì‹œê°„ ì—†ìŒ)
        match = re.search(r'(\d{4})-(\d{2})-(\d{2})', text)
        if match:
            year, month, day = map(int, match.groups())
            try:
                return datetime(year, month, day)
            except:
                pass
        
        # MM.DD í˜•ì‹ (ì˜¬í•´)
        match = re.search(r'(\d{2})\.(\d{2})', text)
        if match:
            month, day = map(int, match.groups())
            try:
                return datetime(now.year, month, day)
            except:
                pass
        
        # MM/DD í˜•ì‹ (ì˜¬í•´)
        match = re.search(r'(\d{2})/(\d{2})', text)
        if match:
            month, day = map(int, match.groups())
            try:
                return datetime(now.year, month, day)
            except:
                pass
        
        # "ì˜¤ëŠ˜", "ì–´ì œ" ë“± ìƒëŒ€ì  ë‚ ì§œ
        text_lower = text.lower()
        if 'ì˜¤ëŠ˜' in text_lower or 'today' in text_lower:
            return now
        if 'ì–´ì œ' in text_lower or 'yesterday' in text_lower:
            return now - timedelta(days=1)
        if 'ë°©ê¸ˆ' in text_lower:
            return now
        
        # "Nì‹œê°„ ì „", "Në¶„ ì „"
        match = re.search(r'(\d+)\s*ì‹œê°„\s*ì „', text)
        if match:
            hours = int(match.group(1))
            return now - timedelta(hours=hours)
        
        match = re.search(r'(\d+)\s*ë¶„\s*ì „', text)
        if match:
            minutes = int(match.group(1))
            return now - timedelta(minutes=minutes)
        
        return None
    
    def crawl_article_list(
        self,
        cafe_url: str,
        keywords: List[str] = None,
        start_date: datetime = None,
        end_date: datetime = None,
        exclude_boards: List[str] = None,
        max_pages: int = 50
    ) -> List[Dict]:
        """
        ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ (í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰)
        
        Args:
            cafe_url: ì¹´í˜ ë©”ì¸ URL (ì˜ˆ: https://cafe.naver.com/imsanbu)
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ë‚˜ëŠ”ì†”ë¡œ", "ë‚˜ì†”"])
            start_date: ì‹œì‘ ë‚ ì§œ (ì—†ìœ¼ë©´ ìµœê·¼ 2ì¼)
            end_date: ì¢…ë£Œ ë‚ ì§œ (ì—†ìœ¼ë©´ ì˜¤ëŠ˜)
            exclude_boards: ì œì™¸í•  ê²Œì‹œíŒëª… ë¦¬ìŠ¤íŠ¸
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        
        Returns:
            ê²Œì‹œê¸€ ëª©ë¡ (post_id, url, title, date, member_id, nickname, board_name)
        """
        if not self.driver:
            print("[Naver Cafe Crawler] âŒ ë¸Œë¼ìš°ì €ê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
            return []
        
        keywords = keywords or []
        if not keywords:
            print("[Naver Cafe Crawler] âš ï¸ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", file=sys.stderr)
            return []
        
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì • (ìµœê·¼ 24ì‹œê°„)
        if not end_date:
            end_date = datetime.now()
        if not start_date:
            start_date = end_date - timedelta(hours=24)  # 24ì‹œê°„ ì „
        
        exclude_boards = exclude_boards or []
        exclude_boards_lower = [b.lower().replace(' ', '') for b in exclude_boards]
        
        # ì¹´í˜ ID ì¶”ì¶œ (ë¬¸ìì—´ ID)
        cafe_id_match = re.search(r'cafe\.naver\.com/([^/?]+)', cafe_url)
        cafe_id = cafe_id_match.group(1) if cafe_id_match else None
        if not cafe_id:
            print(f"[Naver Cafe Crawler] âŒ ì¹´í˜ IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cafe_url}", file=sys.stderr)
            return []
        
        # ì¹´í˜ ë©”ì¸ í˜ì´ì§€ì—ì„œ ìˆ«ì clubid ì¶”ì¶œ
        print(f"[Naver Cafe Crawler] ì¹´í˜ ë©”ì¸ í˜ì´ì§€ì—ì„œ clubid ì¶”ì¶œ ì¤‘...", file=sys.stderr)
        clubid = self._extract_clubid_from_cafe(cafe_url)
        if not clubid:
            print(f"[Naver Cafe Crawler] âš ï¸ ì´ ì¹´í˜ëŠ” ê°€ì…ì´ í•„ìš”í•˜ê±°ë‚˜ ë¹„ê³µê°œ ì¹´í˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ì¹´í˜ë¡œ ì´ë™í•©ë‹ˆë‹¤.", file=sys.stderr)
            print(f"[Naver Cafe Crawler] ğŸ’¡ íŒ: í¬ë¡¤ë§ ì „ì— ì¹´í˜ì— ê°€ì…í•´ë‘ë©´ ë” ë§ì€ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", file=sys.stderr)
            return []
        
        print(f"[Naver Cafe Crawler] âœ… clubid: {clubid} (ì¹´í˜: {cafe_id})", file=sys.stderr)
        
        all_posts = []
        
        try:
            # ê° í‚¤ì›Œë“œë³„ë¡œ ê²€ìƒ‰
            for keyword in keywords:
                if len(all_posts) >= 1000:  # ì•ˆì „ì¥ì¹˜
                    break
                
                print(f"[Naver Cafe Crawler] í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘... (ì¹´í˜: {cafe_id})", file=sys.stderr)
                
                # ë„¤ì´ë²„ ì¹´í˜ ê²€ìƒ‰ URL ìƒì„± (2026ë…„ ìµœì‹  SPA ë°©ì‹)
                from urllib.parse import quote
                encoded_keyword = quote(keyword)
                
                # Gemini ì „ëµ A: ëª©ë¡ ë·° ê°•ì œ (ë‚ ì§œ ì •ë³´ í‘œì‹œ)
                # viewType=L: ë¦¬ìŠ¤íŠ¸ ë·°
                # userDisplay=15: 15ê°œì”© í‘œì‹œ
                # sortBy=date: ìµœì‹ ìˆœ ì •ë ¬
                search_url = f"https://cafe.naver.com/f-e/cafes/{clubid}/menus/0?viewType=L&userDisplay=15&sortBy=date&page=1&q={encoded_keyword}"
                
                print(f"[Naver Cafe Crawler] ê²€ìƒ‰ URL (ëª©ë¡ ë·° ê°•ì œ): {search_url}", file=sys.stderr)
                print(f"[Naver Cafe Crawler] Gemini ì „ëµ A: ë¦¬ìŠ¤íŠ¸ ë·° ê°•ì œë¡œ ë‚ ì§œ ì»¬ëŸ¼ í‘œì‹œ", file=sys.stderr)
                
                # ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
                self.driver.get(search_url)
                
                # SPA ë°©ì‹ì€ iframeì´ ì—†ì„ ìˆ˜ ìˆìŒ (URLì— /f-e/ í¬í•¨ ì—¬ë¶€ë¡œ íŒë‹¨)
                if '/f-e/' not in search_url:
                        # êµ¬ì‹ iframe ë°©ì‹ë§Œ iframe ì „í™˜ ì‹œë„
                    time.sleep(2)
                    self.switch_to_iframe_if_needed()
                else:
                    print("[Naver Cafe Crawler] SPA ë°©ì‹ - JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘...", file=sys.stderr)
                    # SPA JavaScript ì‹¤í–‰ ì‹œê°„ (ë‹¨ì¶•)
                    time.sleep(3)
                    
                    # í˜ì´ì§€ ì™„ì „ ë¡œë“œ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
                    try:
                        WebDriverWait(self.driver, 5).until(
                            lambda d: d.execute_script("return document.readyState") == "complete"
                        )
                        print("[Naver Cafe Crawler] âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ", file=sys.stderr)
                    except:
                        pass
                    
                    # ì¶”ê°€ ì•ˆì •í™” ëŒ€ê¸° (ë‹¨ì¶•)
                    time.sleep(1)
                
                # should_continue_page ë³€ìˆ˜ ì´ˆê¸°í™” (í‚¤ì›Œë“œ ë ˆë²¨)
                should_continue_page = True
                
                # í˜ì´ì§€ ì†ŒìŠ¤ í™•ì¸ (ë””ë²„ê¹…)
                page_source = self.driver.page_source
                print(f"[Naver Cafe Crawler] í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ. í˜ì´ì§€ ê¸¸ì´: {len(page_source)}", file=sys.stderr)
                
                # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
                try:
                    no_result = self.driver.find_elements(By.CSS_SELECTOR, ".nodata, .no_result, .empty, .no-data, .search_no_result")
                    if no_result:
                        print(f"[Naver Cafe Crawler] í‚¤ì›Œë“œ '{keyword}': ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", file=sys.stderr)
                        continue
                except:
                    pass  # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìŒ
                
                # í˜ì´ì§€ë³„ë¡œ í¬ë¡¤ë§
                for page in range(1, max_pages + 1):
                    if len(all_posts) >= 1000:
                        break
                    
                    # í˜ì´ì§€ URL (ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ë„¤ì´ì…˜)
                    if page > 1:
                        # SPA ë°©ì‹: page íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (Gemini ì „ëµ A ìœ ì§€)
                        if '/f-e/' in search_url:
                            page_url = search_url.replace(f'page=1', f'page={page}')
                        else:
                            # êµ¬ì‹ ë°©ì‹
                            page_url = f"{search_url}&search.page={page}"
                        
                        print(f"[Naver Cafe Crawler] í˜ì´ì§€ {page}ë¡œ ì´ë™: {page_url}", file=sys.stderr)
                        self.driver.get(page_url)
                        
                        # iframe ì „í™˜ (êµ¬ì‹ ë°©ì‹ë§Œ)
                        if '/f-e/' not in page_url:
                            time.sleep(2)
                            self.switch_to_iframe_if_needed()
                        else:
                            # SPA í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë‹¨ì¶•)
                            print(f"[Naver Cafe Crawler] SPA í˜ì´ì§€ {page} ë¡œë”© ëŒ€ê¸°...", file=sys.stderr)
                            time.sleep(3)
                            try:
                                WebDriverWait(self.driver, 5).until(
                                    lambda d: d.execute_script("return document.readyState") == "complete"
                                )
                            except:
                                pass
                            time.sleep(1)
                    
                    # ìŠ¤í¬ë¡¤ (SPAëŠ” ìŠ¤í¬ë¡¤ì´ ì¤‘ìš”í•¨)
                    self.driver.execute_script("window.scrollTo(0, 1000);")
                    time.sleep(2)
                    
                    # [ì¤‘ìš”] í…Œì´ë¸” í—¤ë”ì—ì„œ "ì‘ì„±ì¼" ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
                    date_column_index = None
                    try:
                        headers = self.driver.find_elements(By.CSS_SELECTOR, "thead th, thead td")
                        if not headers:
                            headers = self.driver.find_elements(By.CSS_SELECTOR, "tr:first-child th, tr:first-child td")
                        
                        for idx, header in enumerate(headers):
                            header_text = header.text.strip()
                            if 'ì‘ì„±ì¼' in header_text or 'ë‚ ì§œ' in header_text or 'ë“±ë¡ì¼' in header_text:
                                date_column_index = idx
                                print(f"[Naver Cafe Crawler] âœ… 'ì‘ì„±ì¼' ì»¬ëŸ¼ ë°œê²¬: {idx+1}ë²ˆì§¸ ì»¬ëŸ¼", file=sys.stderr)
                                break
                    except:
                        pass
                    
                    # SPA ë°©ì‹: ê²Œì‹œê¸€ ë¡œë”© ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
                    if '/f-e/' in self.driver.current_url:
                        print("[Naver Cafe Crawler] SPA ê²Œì‹œê¸€ ë¡œë”© ëŒ€ê¸° ì¤‘...", file=sys.stderr)
                        
                        # ê²Œì‹œê¸€ ë¡œë“œ í™•ì¸ (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
                        loaded = False
                        try:
                            # ë°©ë²• 1: ê²Œì‹œê¸€ ë§í¬ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸° (5ì´ˆë¡œ ë‹¨ì¶•)
                            WebDriverWait(self.driver, 5).until(
                                lambda d: len(d.find_elements(By.CSS_SELECTOR, "a[href*='articles']")) > 0
                            )
                            loaded = True
                            print("[Naver Cafe Crawler] âœ… SPA ê²Œì‹œê¸€ ë¡œë“œ ì™„ë£Œ (ë§í¬ ë°œê²¬)", file=sys.stderr)
                        except TimeoutException:
                            print("[Naver Cafe Crawler] âš ï¸ ë°©ë²• 1 ì‹¤íŒ¨ - ë‹¤ë¥¸ ì…€ë ‰í„° ì‹œë„", file=sys.stderr)
                        
                        if not loaded:
                            try:
                                # ë°©ë²• 2: ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ëŒ€ê¸° (3ì´ˆë¡œ ë‹¨ì¶•)
                                WebDriverWait(self.driver, 3).until(
                                    lambda d: len(d.find_elements(By.CSS_SELECTOR, "div[class*='ArticleList'], ul[class*='article'], div.article-board")) > 0
                                )
                                loaded = True
                                print("[Naver Cafe Crawler] âœ… SPA ê²Œì‹œê¸€ ë¡œë“œ ì™„ë£Œ (ë¦¬ìŠ¤íŠ¸ ë°œê²¬)", file=sys.stderr)
                            except TimeoutException:
                                print("[Naver Cafe Crawler] âš ï¸ ë°©ë²• 2 ì‹¤íŒ¨ - ê°•ì œ ëŒ€ê¸°", file=sys.stderr)
                        
                        # ìµœì¢… ì•ˆì •í™” ëŒ€ê¸° (2ì´ˆë¡œ ë‹¨ì¶•)
                        time.sleep(2)
                        
                        # í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´ í™•ì¸ (ë””ë²„ê¹…)
                        page_length = len(self.driver.page_source)
                        print(f"[Naver Cafe Crawler] í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´: {page_length}ì", file=sys.stderr)
                        if page_length < 1000:
                            print(f"[Naver Cafe Crawler] âš ï¸ í˜ì´ì§€ê°€ ë„ˆë¬´ ì§§ìŒ - JavaScript ë¯¸ì‹¤í–‰ ê°€ëŠ¥ì„±", file=sys.stderr)
                    
                    # ê²Œì‹œê¸€ í–‰ ì°¾ê¸° (2026ë…„ ìµœì‹  ë„¤ì´ë²„ ì¹´í˜ SPA êµ¬ì¡°)
                    rows = []
                    selectors = [
                        # [ìµœìš°ì„ ] 2026ë…„ SPA êµ¬ì¡° - ë§í¬ ê¸°ë°˜ìœ¼ë¡œ ì—­ì¶”ì 
                        # ê²Œì‹œê¸€ ë§í¬ë¥¼ í¬í•¨í•˜ëŠ” ë¶€ëª¨ ìš”ì†Œ ì°¾ê¸°
                        "*:has(> a[href*='/articles/'])",
                        "*:has(a[href*='/articles/'])",
                        
                        # [SPA ë¦¬ìŠ¤íŠ¸í˜•] ì¼ë°˜ì ì¸ SPA êµ¬ì¡°
                        "ul[class*='ArticleList'] > li",
                        "ul[class*='list'] > li",
                        "div[class*='ArticleList'] > div",
                        "div[class*='list-item']",
                        "div[class*='article-item']",
                        
                        # [Gemini ì œì•ˆ] êµ¬ì‹ í…Œì´ë¸” êµ¬ì¡°
                        "div.article-board > table > tbody > tr",
                        "div.article-board table tbody tr",
                        "table.board-list tbody tr",
                        "#main-area table tbody tr",
                        "table[class*='article'] tbody tr",
                        
                        # [ë²”ìš©] í…Œì´ë¸”
                        "table tbody tr",
                        "div[id*='cafe_main'] table tbody tr",
                        
                        # [ë¦¬ìŠ¤íŠ¸í˜•] ì¹´í˜ë³„ ëŒ€ì²´ êµ¬ì¡°
                        "div.article-board li.board_box",
                        "ul.article_list > li",
                        "div.list_area li",
                        "tr[align='center']",
                        
                        # [ìµœí›„] ë™ì  í´ë˜ìŠ¤ ë° ë²”ìš©
                        "div[class*='ArticleItem']",
                        "li[class*='article']",
                        "div[class*='Item']",
                        "li[class*='item']"
                    ]
                    
                    # [ë°©ë²• 1] ë¨¼ì € ê²Œì‹œê¸€ ë§í¬ë¥¼ ì°¾ì•„ì„œ ë¶€ëª¨ ìš”ì†Œ ìˆ˜ì§‘ (SPA ìµœì í™”)
                    if '/f-e/' in self.driver.current_url and not rows:
                        try:
                            print("[Naver Cafe Crawler] SPA ë°©ì‹: ê²Œì‹œê¸€ ë§í¬ì—ì„œ ë¶€ëª¨ ìš”ì†Œ ì¶”ì¶œ...", file=sys.stderr)
                            article_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/articles/']")
                            if article_links:
                                # ê° ë§í¬ì˜ ë¶€ëª¨ ìš”ì†Œë¥¼ rowë¡œ ì‚¬ìš©
                                parent_rows = []
                                for link in article_links:
                                    try:
                                        # ë¶€ëª¨ ìš”ì†Œ ì¤‘ ì ì ˆí•œ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ìµœëŒ€ 5ë‹¨ê³„)
                                        parent = link
                                        for _ in range(5):
                                            parent = parent.find_element(By.XPATH, "..")
                                            # ë¶€ëª¨ê°€ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì´ë‚˜ í–‰ì´ë©´ ì¶”ê°€
                                            tag = parent.tag_name.lower()
                                            if tag in ['li', 'tr', 'div'] and parent not in parent_rows:
                                                parent_rows.append(parent)
                                                break
                                    except:
                                        continue
                                
                                if parent_rows:
                                    rows = parent_rows
                                    print(f"[Naver Cafe Crawler] âœ… ë§í¬ ê¸°ë°˜ìœ¼ë¡œ {len(rows)}ê°œ ë¶€ëª¨ ìš”ì†Œ ë°œê²¬", file=sys.stderr)
                        except Exception as e:
                            print(f"[Naver Cafe Crawler] ë§í¬ ê¸°ë°˜ ì¶”ì¶œ ì˜¤ë¥˜: {e}", file=sys.stderr)
                    
                    # [ë°©ë²• 2] ì¼ë°˜ ì…€ë ‰í„°ë¡œ ì‹œë„
                    if not rows:
                        for selector in selectors:
                            try:
                                # :has() ì…€ë ‰í„°ëŠ” ê±´ë„ˆë›°ê¸° (Selenium ë¯¸ì§€ì›)
                                if ':has(' in selector:
                                    continue
                                
                                rows = self.driver.find_elements(By.CSS_SELECTOR, selector)
                                if rows and len(rows) > 1:  # ìµœì†Œ 2ê°œ ì´ìƒ (í—¤ë” ì œì™¸)
                                    print(f"[Naver Cafe Crawler] âœ… ì…€ë ‰í„° '{selector}'ë¡œ {len(rows)}ê°œ í–‰ ë°œê²¬", file=sys.stderr)
                                    break
                                elif rows:
                                    print(f"[Naver Cafe Crawler] âš ï¸ ì…€ë ‰í„° '{selector}'ë¡œ {len(rows)}ê°œë§Œ ë°œê²¬ (ê³„ì† ì‹œë„)", file=sys.stderr)
                            except Exception as e:
                                print(f"[Naver Cafe Crawler] ì…€ë ‰í„° '{selector}' ì˜¤ë¥˜: {e}", file=sys.stderr)
                                continue
                    
                    if not rows:
                        print(f"[Naver Cafe Crawler] âš ï¸ í˜ì´ì§€ {page}: ê²Œì‹œê¸€ 0ê°œ - ìë™ ë””ë²„ê¹… ì‹œì‘", file=sys.stderr)
                        
                        # [ìë™ ë””ë²„ê¹… 1] ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                        self.save_debug_screenshot(f"no_posts_page_{page}")
                        
                        # [ìë™ ë””ë²„ê¹… 2] HTML ì†ŒìŠ¤ ì €ì¥
                        self.save_page_source(f"no_posts_page_{page}")
                        
                        # [ìë™ ë””ë²„ê¹… 3] í˜„ì¬ ìƒíƒœ ì¶œë ¥
                        print(f"[DEBUG] í˜„ì¬ URL: {self.driver.current_url}", file=sys.stderr)
                        
                        # [ìë™ ë””ë²„ê¹… 4] iframe ìƒíƒœ í™•ì¸
                        try:
                            current_frame = self.driver.execute_script("return self.name")
                            print(f"[DEBUG] í˜„ì¬ í”„ë ˆì„: {current_frame if current_frame else 'default'}", file=sys.stderr)
                        except:
                            print(f"[DEBUG] í”„ë ˆì„ ì •ë³´ í™•ì¸ ì‹¤íŒ¨", file=sys.stderr)
                        
                        # [ìë™ ë””ë²„ê¹… 5] í˜ì´ì§€ ë‚´ ëª¨ë“  í…Œì´ë¸” êµ¬ì¡° ì¶œë ¥
                        try:
                            all_tables = self.driver.find_elements(By.TAG_NAME, "table")
                            print(f"[DEBUG] í˜ì´ì§€ ë‚´ í…Œì´ë¸” ê°œìˆ˜: {len(all_tables)}", file=sys.stderr)
                            for i, table in enumerate(all_tables[:3]):  # ìµœëŒ€ 3ê°œë§Œ
                                table_class = table.get_attribute('class') or 'no-class'
                                table_id = table.get_attribute('id') or 'no-id'
                                print(f"[DEBUG] í…Œì´ë¸” {i+1}: class='{table_class}', id='{table_id}'", file=sys.stderr)
                                
                                # í…Œì´ë¸” ë‚´ tr ê°œìˆ˜
                                try:
                                    trs = table.find_elements(By.TAG_NAME, "tr")
                                    print(f"[DEBUG]   â””â”€ tr ê°œìˆ˜: {len(trs)}", file=sys.stderr)
                                except:
                                    pass
                        except Exception as e:
                            print(f"[DEBUG] í…Œì´ë¸” ë¶„ì„ ì˜¤ë¥˜: {e}", file=sys.stderr)
                        
                        # [ìë™ ë””ë²„ê¹… 6] ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸° ì‹œë„ (SPA)
                        try:
                            article_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/articles/']")
                            print(f"[DEBUG] ê²Œì‹œê¸€ ë§í¬ (a[href*='/articles/']) ê°œìˆ˜: {len(article_links)}", file=sys.stderr)
                            if article_links:
                                for i, link in enumerate(article_links[:3]):  # ìµœëŒ€ 3ê°œë§Œ
                                    print(f"[DEBUG] ë§í¬ {i+1}: {link.get_attribute('href')}", file=sys.stderr)
                                    try:
                                        print(f"[DEBUG]   í…ìŠ¤íŠ¸: {link.text[:50]}", file=sys.stderr)
                                    except:
                                        pass
                        except Exception as e:
                            print(f"[DEBUG] ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸° ì˜¤ë¥˜: {e}", file=sys.stderr)
                        
                        # [ìë™ ë””ë²„ê¹… 7] ëª¨ë“  ë§í¬ í™•ì¸
                        try:
                            all_links = self.driver.find_elements(By.TAG_NAME, "a")
                            print(f"[DEBUG] í˜ì´ì§€ ë‚´ ì „ì²´ ë§í¬ ê°œìˆ˜: {len(all_links)}", file=sys.stderr)
                            # articles ë˜ëŠ” cafe ê´€ë ¨ ë§í¬ í•„í„°ë§
                            relevant_links = [l for l in all_links if l.get_attribute('href') and 'cafe.naver.com' in l.get_attribute('href')]
                            print(f"[DEBUG] ì¹´í˜ ê´€ë ¨ ë§í¬ ê°œìˆ˜: {len(relevant_links)}", file=sys.stderr)
                        except:
                            pass
                        
                        # [ìë™ ë””ë²„ê¹… 8] í˜ì´ì§€ ì†ŒìŠ¤ ìƒ˜í”Œ
                        page_source_sample = self.driver.page_source[:2000]
                        print(f"[DEBUG] í˜ì´ì§€ ì†ŒìŠ¤ ìƒ˜í”Œ (2000ì):\n{page_source_sample}\n", file=sys.stderr)
                        
                        print(f"[Naver Cafe Crawler] ğŸ’¡ ë””ë²„ê¹… íŒŒì¼ì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì„¸ìš”.", file=sys.stderr)
                        break
                    
                    # í˜ì´ì§€ë³„ should_continue_page í”Œë˜ê·¸ ë¦¬ì…‹
                    page_should_continue = True
                    
                    for row in rows:
                        try:
                            # ê³µì§€/ìƒë‹¨ ê³ ì • ìŠ¤í‚µ
                            try:
                                row_class = row.get_attribute('class') or ''
                                if 'notice' in row_class.lower() or 'top' in row_class.lower():
                                    continue
                            except Exception as e:
                                # StaleElementReferenceException ë“± ë¬´ì‹œ
                                if 'stale element' in str(e).lower():
                                    print(f"[Naver Cafe Crawler] âš ï¸ Stale element ìŠ¤í‚µ (í˜ì´ì§€ ì—…ë°ì´íŠ¸ë¨)", file=sys.stderr)
                                    continue
                                # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ì§„í–‰
                                pass
                            
                            # ì œëª©/URL ì¶”ì¶œ (2026ë…„ ìµœì‹  SPA êµ¬ì¡° í¬í•¨)
                            title = ''
                            href = ''
                            title_selectors = [
                                # [ìµœìš°ì„ ] 2026ë…„ SPA ë°©ì‹
                                "a[href*='/articles/']",  # SPA ê²Œì‹œê¸€ ë§í¬
                                "a[href*='/f-e/cafes/']",  # SPA ì „ì²´ ë§í¬
                                
                                # [ìš°ì„ ] Gemini ì œì•ˆ - êµ¬ì‹ ë°©ì‹
                                "a.article",
                                "td a.article",
                                
                                # [ë°±ì—…] ê²€ì¦ëœ ë§í¬ ì…€ë ‰í„°
                                "a[href*='ArticleRead']",
                                "a[href*='articleid']",
                                
                                # [ì¼ë°˜] ì œëª© ì…€ë ‰í„°
                                "td.title a",
                                "td.subject a",
                                ".title a",
                                ".subject a",
                                "a.title",
                                "a.subject",
                                "td.board-list a.article",
                                
                                # [ìµœí›„] ëª¨ë“  ë§í¬
                                "td a",
                                "a"
                            ]
                            
                            for selector in title_selectors:
                                try:
                                    title_elems = row.find_elements(By.CSS_SELECTOR, selector)
                                    for title_elem in title_elems:
                                        temp_title = title_elem.text.strip()
                                        temp_href = title_elem.get_attribute('href')
                                        # SPA ë°©ì‹ ë˜ëŠ” êµ¬ì‹ ë°©ì‹ ë§í¬ í™•ì¸
                                        if temp_title and temp_href and (
                                            '/articles/' in temp_href or  # SPA
                                            'ArticleRead' in temp_href or  # êµ¬ì‹
                                            'articleid' in temp_href       # êµ¬ì‹
                                        ):
                                            title = temp_title
                                            href = temp_href
                                            break
                                    if title and href:
                                        break
                                except:
                                    continue
                            
                            if not href or not title:
                                # ë””ë²„ê¹…: í–‰ì˜ ëª¨ë“  ë§í¬ ì¶œë ¥
                                try:
                                    all_links = row.find_elements(By.TAG_NAME, "a")
                                    print(f"[Naver Cafe Crawler] ì œëª©/URL ì¶”ì¶œ ì‹¤íŒ¨. í–‰ì—ì„œ ë°œê²¬ëœ ë§í¬ {len(all_links)}ê°œ", file=sys.stderr)
                                    for link in all_links[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                                        print(f"  - {link.text[:30]}: {link.get_attribute('href')}", file=sys.stderr)
                                except:
                                    pass
                                continue
                            
                            # URL ì •ê·œí™”
                            normalized_url = self.normalize_article_url(href)
                            if not normalized_url:
                                continue
                            
                            # post_id ì¶”ì¶œ (SPA ë° PC í‘œì¤€ ëª¨ë‘ ì§€ì›)
                            post_id = None
                            # SPA í˜•ì‹: /articles/789012
                            match = re.search(r'/articles/(\d+)', normalized_url)
                            if match:
                                post_id = match.group(1)
                            else:
                                # PC í‘œì¤€ í˜•ì‹: articleid=789012
                                match = re.search(r'articleid=(\d+)', normalized_url)
                                if match:
                                    post_id = match.group(1)
                            
                            if not post_id:
                                continue
                            
                            # [2026ë…„ ìµœì‹ ] SPA ê²€ìƒ‰ ê²°ê³¼ì—ëŠ” ë‚ ì§œê°€ HTMLì— ì—†ìŒ
                            # â†’ ëª¨ë“  ê²Œì‹œê¸€ì„ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ í™•ì¸
                            
                            # ë³€ìˆ˜ ì´ˆê¸°í™”
                            date_text = ''
                            detail_info = None
                            
                            # [ë””ë²„ê¹…] ì²« ë²ˆì§¸ ê²Œì‹œê¸€ì˜ HTML êµ¬ì¡° ì¶œë ¥
                            if len(all_posts) == 0 and page == 1:
                                try:
                                    row_html = row.get_attribute('outerHTML')
                                    print(f"\n[DEBUG] ì²« ë²ˆì§¸ ê²Œì‹œê¸€ HTML êµ¬ì¡°:\n{row_html[:1000]}\n", file=sys.stderr)
                                    print(f"[DEBUG] ê²Œì‹œê¸€ ì „ì²´ í…ìŠ¤íŠ¸:\n{row.text}\n", file=sys.stderr)
                                    print(f"[DEBUG] âš ï¸ SPA ê²€ìƒ‰ ê²°ê³¼ì—ëŠ” ë‚ ì§œê°€ ì—†ìŒ â†’ ìƒì„¸ í˜ì´ì§€ì—ì„œ í™•ì¸", file=sys.stderr)
                                except:
                                    pass
                            
                            # [ë°©ë²• 1] í…Œì´ë¸” êµ¬ì¡°: ì‘ì„±ì¼ ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì‚¬ìš©
                            if date_column_index is not None:
                                try:
                                    tds = row.find_elements(By.TAG_NAME, "td")
                                    if len(tds) > date_column_index:
                                        temp_text = tds[date_column_index].text.strip()
                                        if temp_text:
                                            date_text = temp_text
                                            print(f"[DEBUG] âœ… ì‘ì„±ì¼ ì»¬ëŸ¼ì—ì„œ ë‚ ì§œ ë°œê²¬: '{date_text}'", file=sys.stderr)
                                except Exception as e:
                                    if len(all_posts) == 0 and page == 1:
                                        print(f"[DEBUG] ì‘ì„±ì¼ ì»¬ëŸ¼ ì¶”ì¶œ ì˜¤ë¥˜: {e}", file=sys.stderr)
                            
                            # [ë°©ë²• 2] SPA êµ¬ì¡°: íŠ¹ì • ì…€ë ‰í„°ë¡œ ë‚ ì§œ ì°¾ê¸°
                            if not date_text:
                                date_selectors = [
                                    # [ìµœìš°ì„ ] ì‘ì„±ì¼ (ë„¤ì´ë²„ ì¹´í˜ í‘œì¤€)
                                    "td[aria-label*='ì‘ì„±ì¼']",
                                    "div[aria-label*='ì‘ì„±ì¼']",
                                    "span[aria-label*='ì‘ì„±ì¼']",
                                    "*[aria-label*='ì‘ì„±ì¼']",
                                    
                                    # [SPA ìµœì‹ ] 2026ë…„ êµ¬ì¡°
                                    "span[class*='date']",
                                    "div[class*='date']",
                                    "time",
                                    "span[class*='Date']",
                                    "div[class*='Date']",
                                    
                                    # [êµ¬ì‹] í…Œì´ë¸” êµ¬ì¡°
                                    "td.td_date",
                                    "td.td_normal",  # ì¼ë°˜ ê²Œì‹œíŒ ë‚ ì§œ ì»¬ëŸ¼
                                    "td[class*='date']",
                                    ".date",
                                    
                                    # [ë°±ì—…] ìœ„ì¹˜ ê¸°ë°˜ (ì œëª© ë‹¤ìŒ, ì¡°íšŒìˆ˜ ì´ì „)
                                    "td:nth-child(3)",  # ë³´í†µ 3ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‘ì„±ì¼
                                    "td:nth-last-child(2)",  # ëì—ì„œ 2ë²ˆì§¸ (ì¡°íšŒìˆ˜ ì´ì „)
                                ]
                            
                            if not date_text:
                                for selector in date_selectors:
                                    try:
                                        date_elems = row.find_elements(By.CSS_SELECTOR, selector)
                                        
                                        # [ë””ë²„ê¹…] ì²« ë²ˆì§¸ ê²Œì‹œê¸€ì˜ ë‚ ì§œ ìš”ì†Œ í™•ì¸
                                        if len(all_posts) == 0 and page == 1 and date_elems:
                                            print(f"[DEBUG] ì…€ë ‰í„° '{selector}'ë¡œ {len(date_elems)}ê°œ ìš”ì†Œ ë°œê²¬:", file=sys.stderr)
                                            for i, elem in enumerate(date_elems[:3]):
                                                print(f"  [{i+1}] í…ìŠ¤íŠ¸: '{elem.text}'", file=sys.stderr)
                                        
                                        for date_elem in date_elems:
                                            temp_text = date_elem.text.strip() if date_elem else ''
                                            # ë‚ ì§œ í˜•ì‹ë§Œ ì¶”ì¶œ (ìˆ«ì íŒ¨í„´ í™•ì¸)
                                            if temp_text and (
                                                re.search(r'\d{4}[./]\d{2}[./]\d{2}', temp_text) or 
                                                re.search(r'\d{2}[./]\d{2}', temp_text) or
                                                re.search(r'\d{4}\.\d{2}\.\d{2}', temp_text) or
                                                'ì˜¤ëŠ˜' in temp_text or 
                                                'ì–´ì œ' in temp_text or
                                                re.search(r'\d{2}:\d{2}', temp_text)  # ì‹œê°„ í˜•ì‹
                                            ):
                                                # ëŒ“ê¸€ìˆ˜ê°€ ì•„ë‹Œì§€ í™•ì¸
                                                if 'ëŒ“ê¸€' not in temp_text and '[' not in temp_text and 'ì¡°íšŒ' not in temp_text:
                                                    date_text = temp_text
                                                    if len(all_posts) == 0 and page == 1:
                                                        print(f"[DEBUG] âœ… ë‚ ì§œ ë°œê²¬: '{date_text}' (ì…€ë ‰í„°: {selector})", file=sys.stderr)
                                                    break
                                        if date_text:
                                            break
                                    except Exception as e:
                                        if len(all_posts) == 0 and page == 1:
                                            print(f"[DEBUG] ì…€ë ‰í„° '{selector}' ì˜¤ë¥˜: {e}", file=sys.stderr)
                                        continue
                            
                            # [ë°©ë²• 3] í–‰ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ (Gemini ì „ëµ C)
                            if not date_text:
                                try:
                                    # innerText ì „ì²´ ê°€ì ¸ì˜¤ê¸°
                                    row_text = row.text
                                    
                                    if len(all_posts) == 0 and page == 1:
                                        print(f"[DEBUG] í–‰ ì „ì²´ í…ìŠ¤íŠ¸:\n{row_text}", file=sys.stderr)
                                    
                                    # "ì‘ì„±ì¼" ê·¼ì²˜ì˜ í…ìŠ¤íŠ¸ ìš°ì„  í™•ì¸
                                    if 'ì‘ì„±ì¼' in row_text:
                                        match = re.search(r'ì‘ì„±ì¼[^\d]*([\d.:/-]+)', row_text)
                                        if match:
                                            date_text = match.group(1)
                                            if len(all_posts) == 0 and page == 1:
                                                print(f"[DEBUG] âœ… 'ì‘ì„±ì¼' ê·¼ì²˜ì—ì„œ ë‚ ì§œ ë°œê²¬: '{date_text}'", file=sys.stderr)
                                    
                                    # Gemini ì œì•ˆ: ë‹¤ì–‘í•œ ë‚ ì§œ íŒ¨í„´ (ìƒëŒ€ì  ì‹œê°„ í¬í•¨)
                                    if not date_text:
                                        date_patterns = [
                                            (r'\d{4}\.\d{2}\.\d{2}', 'ì™„ì „ ë‚ ì§œ'),      # 2024.01.15
                                            (r'\d{4}-\d{2}-\d{2}', 'ì™„ì „ ë‚ ì§œ'),        # 2024-01-15
                                            (r'\d{2}\.\d{2}\.', 'ì›”ì¼ ë‚ ì§œ'),           # 01.15.
                                            (r'\d{2}\.\d{2}(?!\d)', 'ì›”ì¼ ë‚ ì§œ'),       # 01.15 (ë’¤ì— ìˆ«ì ì—†ìŒ)
                                            (r'\d{2}/\d{2}', 'ì›”ì¼ ìŠ¬ë˜ì‹œ'),            # 01/15
                                            (r'\d{1,2}:\d{2}', 'ì‹œê°„'),                 # 14:30 ë˜ëŠ” 9:15
                                            (r'\d+ì‹œê°„\s*ì „', 'ìƒëŒ€ ì‹œê°„'),              # 2ì‹œê°„ ì „
                                            (r'\d+ë¶„\s*ì „', 'ìƒëŒ€ ë¶„'),                 # 30ë¶„ ì „
                                            (r'ë°©ê¸ˆ', 'ë°©ê¸ˆ'),                          # ë°©ê¸ˆ
                                        ]
                                        
                                        # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê° ì¤„ì—ì„œ ì°¾ê¸°
                                        lines = row_text.split('\n')
                                        for line in lines:
                                            # ëŒ“ê¸€ìˆ˜ë‚˜ ì¡°íšŒìˆ˜ê°€ ì•„ë‹Œ ì¤„ì—ì„œë§Œ ì°¾ê¸°
                                            if 'ëŒ“ê¸€' in line or 'ì¡°íšŒ' in line or '[' in line or ']' in line:
                                                continue
                                            
                                            for pattern, pattern_name in date_patterns:
                                                match = re.search(pattern, line)
                                                if match:
                                                    date_text = match.group(0)
                                                    if len(all_posts) == 0 and page == 1:
                                                        print(f"[DEBUG] âœ… ì •ê·œì‹ìœ¼ë¡œ ë‚ ì§œ ë°œê²¬: '{date_text}' (íŒ¨í„´: {pattern_name}, ë¼ì¸: '{line[:50]}')", file=sys.stderr)
                                                    break
                                            if date_text:
                                                break
                                    
                                    # ìƒëŒ€ì  ë‚ ì§œ
                                    if not date_text:
                                        if 'ì˜¤ëŠ˜' in row_text:
                                            date_text = 'ì˜¤ëŠ˜'
                                        elif 'ì–´ì œ' in row_text:
                                            date_text = 'ì–´ì œ'
                                        elif 'ë°©ê¸ˆ' in row_text:
                                            date_text = 'ë°©ê¸ˆ'
                                except Exception as e:
                                    if len(all_posts) == 0 and page == 1:
                                        print(f"[DEBUG] ì •ê·œì‹ ì¶”ì¶œ ì˜¤ë¥˜: {e}", file=sys.stderr)
                            
                            # [ì „ë¬¸ê°€ ê²€ì¦] ë‚ ì§œ ì—†ìœ¼ë©´ API ìš°ì„  â†’ ì‹¤íŒ¨ ì‹œ ìƒì„¸ í˜ì´ì§€
                            if not date_text:
                                # 1ë‹¨ê³„: Article APIë¡œ ë‚ ì§œ í™•ë³´ ì‹œë„ (ê°€ì¥ ë¹ ë¦„)
                                print(f"[Naver Cafe Crawler] ë‚ ì§œ ì—†ìŒ - API í™•ì¸ ì‹œë„. ì œëª©: '{title[:30]}...'", file=sys.stderr)
                                
                                # clubid ì¶”ì¶œ (ì´ë¯¸ ì¶”ì¶œë˜ì–´ ìˆì–´ì•¼ í•¨)
                                api_clubid = None
                                if '/cafes/' in normalized_url:
                                    match = re.search(r'/cafes/(\d+)/', normalized_url)
                                    if match:
                                        api_clubid = match.group(1)
                                elif 'clubid=' in normalized_url:
                                    match = re.search(r'clubid=(\d+)', normalized_url)
                                    if match:
                                        api_clubid = match.group(1)
                                
                                if api_clubid and post_id:
                                    api_info = self.get_article_info_from_api(api_clubid, post_id)
                                    if api_info and api_info.get('date'):
                                        date_text = str(api_info.get('date'))
                                        print(f"[Naver Cafe Crawler] âœ… APIì—ì„œ ë‚ ì§œ ë°œê²¬: '{date_text}'", file=sys.stderr)
                                        # APIì—ì„œ ë³¸ë¬¸ë„ ê°€ì ¸ì™”ìœ¼ë©´ ì €ì¥
                                        if api_info.get('content'):
                                            detail_info = api_info
                                
                                # 2ë‹¨ê³„: API ì‹¤íŒ¨ ì‹œ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
                                if not date_text:
                                    print(f"[Naver Cafe Crawler] API ì‹¤íŒ¨ - ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼. ì œëª©: '{title[:30]}...'", file=sys.stderr)
                                    try:
                                        # í˜„ì¬ URL ì €ì¥
                                        current_list_url = self.driver.current_url
                                        
                                        # ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
                                        self.driver.get(normalized_url)
                                        time.sleep(2)
                                        
                                        # iframe ì „í™˜ (í•„ìš”ì‹œ)
                                        if '/f-e/' not in normalized_url:
                                            self.switch_to_iframe_if_needed()
                                        
                                        # [ìµœì í™”] ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ, ë³¸ë¬¸, ëŒ“ê¸€ ëª¨ë‘ ìˆ˜ì§‘
                                        # (ì–´ì°¨í”¼ ìƒì„¸ í˜ì´ì§€ì— ì™”ìœ¼ë‹ˆ ëª¨ë“  ì •ë³´ ìˆ˜ì§‘)
                                        detail_info = self.crawl_article_detail(normalized_url, post_id)
                                        
                                        if detail_info and detail_info.get('date'):
                                            date_text = detail_info.get('date', '')
                                            print(f"[Naver Cafe Crawler] âœ… ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: ë‚ ì§œ='{date_text}', ë³¸ë¬¸={len(detail_info.get('content', ''))}ì", file=sys.stderr)
                                        else:
                                            # crawl_article_detailì´ ì‹¤íŒ¨í•œ ê²½ìš°, ë‚ ì§œë§Œì´ë¼ë„ ì°¾ê¸°
                                            detail_date_selectors = [
                                                ".article_date",
                                                ".date_time",
                                                "span.date",
                                                "div.date",
                                                "time",
                                                "*[class*='Date']",
                                                "*[class*='date']"
                                            ]
                                            
                                            for selector in detail_date_selectors:
                                                try:
                                                    date_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                                                    if date_elem:
                                                        temp_text = date_elem.text.strip()
                                                        if temp_text and re.search(r'\d{4}\.\d{2}\.\d{2}|\d{2}\.\d{2}|\d{2}:\d{2}', temp_text):
                                                            date_text = temp_text
                                                            print(f"[Naver Cafe Crawler] âœ… ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ë°œê²¬: '{date_text}'", file=sys.stderr)
                                                            break
                                                except:
                                                    continue
                                        
                                        # ë¦¬ìŠ¤íŠ¸ë¡œ ë³µê·€
                                        try:
                                            self.driver.get(current_list_url)
                                            time.sleep(2)
                                            if '/f-e/' not in current_list_url:
                                                self.switch_to_iframe_if_needed()
                                        except Exception as e:
                                            print(f"[Naver Cafe Crawler] âš ï¸ ë¦¬ìŠ¤íŠ¸ ë³µê·€ ì˜¤ë¥˜: {e}", file=sys.stderr)
                                
                                    except Exception as e:
                                        print(f"[Naver Cafe Crawler] âš ï¸ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ ì˜¤ë¥˜: {e}", file=sys.stderr)
                                        # ê²€ìƒ‰ ê²°ê³¼ë¡œ ëŒì•„ê°€ê¸° ì‹œë„
                                        try:
                                            self.driver.get(current_list_url)
                                            time.sleep(2)
                                            if '/f-e/' not in current_list_url:
                                                self.switch_to_iframe_if_needed()
                                        except:
                                            pass
                                
                                # ìƒì„¸ í˜ì´ì§€ì—ì„œë„ ë‚ ì§œ ëª» ì°¾ìœ¼ë©´ ìŠ¤í‚µ
                                if not date_text and not detail_info:
                                    print(f"[Naver Cafe Crawler] âš ï¸ ìƒì„¸ í˜ì´ì§€ì—ì„œë„ ë‚ ì§œ ì—†ìŒ - ê²Œì‹œê¸€ ìŠ¤í‚µ. ì œëª©: '{title[:30]}...'", file=sys.stderr)
                                    continue
                            
                            # ë‚ ì§œ íŒŒì‹±
                            date_val = self.parse_date_from_text(date_text)
                            if not date_val:
                                print(f"[Naver Cafe Crawler] âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ - ê²Œì‹œê¸€ ìŠ¤í‚µ. ì œëª©: '{title[:30]}...', ë‚ ì§œ í…ìŠ¤íŠ¸: '{date_text}'", file=sys.stderr)
                                continue
                            
                            # ë‚ ì§œ í•„í„°ë§ (24ì‹œê°„ ë²”ìœ„ ì²´í¬)
                            if date_val > end_date:
                                print(f"[Naver Cafe Crawler] â­ï¸ ë¯¸ë˜ ë‚ ì§œ ìŠ¤í‚µ: {date_val.strftime('%Y-%m-%d %H:%M')} > {end_date.strftime('%Y-%m-%d %H:%M')}", file=sys.stderr)
                                continue
                            
                            if date_val < start_date:
                                print(f"[Naver Cafe Crawler] â¸ï¸ 24ì‹œê°„ ì´ì „ ê²Œì‹œê¸€ ë°œê²¬ - ì´ ì¹´í˜ì—ì„œ ì¤‘ë‹¨", file=sys.stderr)
                                print(f"   ê²Œì‹œê¸€ ë‚ ì§œ: {date_val.strftime('%Y-%m-%d %H:%M')} < ì‹œì‘: {start_date.strftime('%Y-%m-%d %H:%M')}", file=sys.stderr)
                                print(f"[Naver Cafe Crawler] ğŸ’¡ ë‹¤ë¥¸ ì¹´í˜ì—ì„œ ê³„ì† ìˆ˜ì§‘í•©ë‹ˆë‹¤...", file=sys.stderr)
                                page_should_continue = False
                                should_continue_page = False
                                break
                            
                            # í‚¤ì›Œë“œ í•„í„°ë§ (ì œëª©ì— í‚¤ì›Œë“œ í¬í•¨ í™•ì¸)
                            if keyword.lower() not in title.lower():
                                continue
                            
                            # [ìµœì í™”] detail_infoê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                            if detail_info:
                                post_data = {
                                    'post_id': post_id,
                                    'url': normalized_url,
                                    'title': title,
                                    'date': date_val.isoformat(),
                                    'content': detail_info.get('content', ''),
                                    'viewCount': detail_info.get('viewCount', 0),
                                    'nickname': detail_info.get('nickname', 'Unknown'),
                                    'member_id': detail_info.get('member_id'),
                                    'board_name': detail_info.get('board_name', ''),
                                    'comments': detail_info.get('comments', []),
                                    'commentCount': len(detail_info.get('comments', [])),
                                    'cafe_url': cafe_url,
                                    'keyword': keyword
                                }
                                all_posts.append(post_data)
                                print(f"[Naver Cafe Crawler] âœ… ê²Œì‹œê¸€ ì™„ì „ ìˆ˜ì§‘: {title[:50]}... (ë‚ ì§œ: {date_val.strftime('%Y-%m-%d')}, ë³¸ë¬¸: {len(post_data.get('content', ''))}ì, ì¡°íšŒ: {post_data.get('viewCount', 0)}, ëŒ“ê¸€: {post_data.get('commentCount', 0)}ê°œ)", file=sys.stderr)
                            else:
                                # ìƒì„¸ ì •ë³´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥ (ë‚˜ì¤‘ì— ìƒì„¸ ìˆ˜ì§‘)
                                board_name = ''
                                nickname = 'Unknown'
                                view_count = 0
                                
                                try:
                                    board_elem = row.find_element(By.CSS_SELECTOR, "a.board_name, td.td_board a, a[href*='/menus/']")
                                    board_name = board_elem.text.strip()
                                except:
                                    pass
                                
                                try:
                                    nickname_elem = row.find_element(By.CSS_SELECTOR, "a[class*='Nickname'], .nick a, td.td_name a, a[class*='Writer'], .writer a")
                                    nickname = nickname_elem.text.strip()
                                    nickname = re.sub(r'\s*ë‹˜ì˜.*', '', nickname)
                                except:
                                    pass
                                
                                # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ëª©ë¡ í˜ì´ì§€)
                                try:
                                    view_selectors = [
                                        "td.td_view",
                                        "span.view",
                                        "*[class*='view']",
                                        "*[class*='View']"
                                    ]
                                    for selector in view_selectors:
                                        try:
                                            view_elem = row.find_element(By.CSS_SELECTOR, selector)
                                            text = view_elem.text.strip()
                                            match = re.search(r'(\d+)', text.replace(',', ''))
                                            if match:
                                                view_count = int(match.group(1))
                                                break
                                        except:
                                            continue
                                except:
                                    pass
                                
                                all_posts.append({
                                    'post_id': post_id,
                                    'url': normalized_url,
                                    'title': title,
                                    'date': date_val.isoformat(),
                                    'viewCount': view_count,
                                    'nickname': nickname,
                                    'board_name': board_name,
                                    'cafe_url': cafe_url,
                                    'keyword': keyword
                                })
                                print(f"[Naver Cafe Crawler] âœ… ê²Œì‹œê¸€ ìˆ˜ì§‘ (ê¸°ë³¸): {title[:50]}... (ë‚ ì§œ: {date_val.strftime('%Y-%m-%d')}, ì¡°íšŒ: {view_count})", file=sys.stderr)
                            
                        except Exception as e:
                            import traceback
                            print(f"[Naver Cafe Crawler] ê²Œì‹œê¸€ íŒŒì‹± ì˜¤ë¥˜: {e}", file=sys.stderr)
                            print(f"[Naver Cafe Crawler] íŠ¸ë ˆì´ìŠ¤ë°±: {traceback.format_exc()}", file=sys.stderr)
                            continue
                    
                    if not page_should_continue:
                        print(f"[Naver Cafe Crawler] í˜ì´ì§€ {page}ì—ì„œ 24ì‹œê°„ ì´ì „ ê²Œì‹œê¸€ ë°œê²¬ - ë‹¤ìŒ í˜ì´ì§€ ìŠ¤í‚µ", file=sys.stderr)
                        break
                
                keyword_posts_count = len([p for p in all_posts if p.get('keyword') == keyword])
                print(f"[Naver Cafe Crawler] í‚¤ì›Œë“œ '{keyword}': í˜ì´ì§€ {page}ê¹Œì§€ {keyword_posts_count}ê°œ ìˆ˜ì§‘", file=sys.stderr)
                
                if not should_continue_page:
                    print(f"[Naver Cafe Crawler] â¸ï¸ í‚¤ì›Œë“œ '{keyword}': ì´ ì¹´í˜ì—ì„œ 24ì‹œê°„ ì´ë‚´ ê²Œì‹œê¸€ ëª¨ë‘ ìˆ˜ì§‘", file=sys.stderr)
                    break
                
                # í‚¤ì›Œë“œ ê°„ ë”œë ˆì´ (ë‹¨ì¶•)
                time.sleep(1)
            
            print(f"[Naver Cafe Crawler] ğŸ“Š ì´ ì¹´í˜ ìˆ˜ì§‘ ê²°ê³¼: ì´ {len(all_posts)}ê°œ (24ì‹œê°„ ì´ë‚´)", file=sys.stderr)
            
            if len(all_posts) == 0:
                print(f"[Naver Cafe Crawler] âš ï¸ ì´ ì¹´í˜ì—ì„œ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ì—†ìŒ", file=sys.stderr)
                print(f"  - í‚¤ì›Œë“œ: {keywords}", file=sys.stderr)
                print(f"  - ë‚ ì§œ ë²”ìœ„: {start_date.strftime('%Y-%m-%d %H:%M')} ~ {end_date.strftime('%Y-%m-%d %H:%M')}", file=sys.stderr)
                print(f"  ğŸ’¡ ë‹¤ë¥¸ ì¹´í˜ë¡œ ì´ë™í•©ë‹ˆë‹¤...", file=sys.stderr)
            else:
                print(f"[Naver Cafe Crawler] ğŸ“Š í‚¤ì›Œë“œë³„ ìˆ˜ì§‘:", file=sys.stderr)
                for keyword in keywords:
                    keyword_count = len([p for p in all_posts if p.get('keyword') == keyword])
                    if keyword_count > 0:
                        print(f"  - '{keyword}': {keyword_count}ê°œ", file=sys.stderr)
            
            return all_posts
            
        except Exception as e:
            import traceback
            print(f"[Naver Cafe Crawler] ëª©ë¡ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}", file=sys.stderr)
            print(f"[Naver Cafe Crawler] íŠ¸ë ˆì´ìŠ¤ë°±: {traceback.format_exc()}", file=sys.stderr)
            return all_posts
    
    def crawl_article_detail(self, article_url: str, post_id: str = None) -> Optional[Dict]:
        """
        ê²Œì‹œê¸€ ìƒì„¸ ìˆ˜ì§‘ (ë³¸ë¬¸ + ëŒ“ê¸€)
        
        Args:
            article_url: ì •ê·œí™”ëœ ê²Œì‹œê¸€ URL
            post_id: ê²Œì‹œê¸€ ID (ì—†ìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ)
        
        Returns:
            {'content': '...', 'member_id': '...', 'comments': [...]}
        """
        if not self.driver:
            return None
        
        try:
            # URL ì •ê·œí™”
            normalized_url = self.normalize_article_url(article_url)
            if not normalized_url:
                return None
            
            # clubid, articleid ì¶”ì¶œ (SPA ë° PC í‘œì¤€ ëª¨ë‘ ì§€ì›)
            clubid = None
            articleid = None
            
            # SPA í˜•ì‹: /f-e/cafes/123456/articles/789012
            spa_match = re.search(r'/cafes/(\d+)/articles/(\d+)', normalized_url)
            if spa_match:
                clubid, articleid = spa_match.groups()
            else:
                # PC í‘œì¤€ í˜•ì‹: clubid=123456&articleid=789012
                match = re.search(r'clubid=(\d+)', normalized_url)
                clubid = match.group(1) if match else None
                
                match = re.search(r'articleid=(\d+)', normalized_url)
                articleid = match.group(1) if match else post_id
            
            if not clubid or not articleid:
                return None
            
            # ìƒì„¸ í˜ì´ì§€ ì´ë™
            self.driver.get(normalized_url)
            time.sleep(random.uniform(1, 2))
            
            # [ì „ë¬¸ê°€ ê²€ì¦] PC í‘œì¤€ URLì€ ë¬´ì¡°ê±´ iframe ì „í™˜
            if 'ArticleRead.nhn' in normalized_url:
                self.switch_to_iframe_if_needed()
                print("[Naver Cafe Crawler] PC í‘œì¤€ URL - iframe ì „í™˜ ì™„ë£Œ", file=sys.stderr)
            else:
                # SPA ë°©ì‹ë„ iframeì´ ìˆìœ¼ë©´ ì „í™˜ ì‹œë„
                try:
                    iframe = self.driver.find_element(By.ID, "cafe_main")
                    self.driver.switch_to.frame(iframe)
                    print("[Naver Cafe Crawler] SPA URL - iframe ë°œê²¬ ë° ì „í™˜", file=sys.stderr)
                except:
                    print("[Naver Cafe Crawler] SPA URL - iframe ì—†ìŒ", file=sys.stderr)
                    pass
            
            time.sleep(1)  # ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
            
            # ë³¸ë¬¸ ì¶”ì¶œ (2026ë…„ ìµœì‹  SPA í¬í•¨)
            content = ''
            selectors = [
                # [ìµœì‹ ] 2026ë…„ SPA ë°©ì‹
                'div[class*="ArticleContentBox"]',
                'div.article-board',
                'div[class*="article_container"]',
                
                # [ì¼ë°˜] ìŠ¤ë§ˆíŠ¸ì—ë””í„°
                '.se-main-container',
                
                # [êµ¬ì‹] PC í‘œì¤€
                '#articleBody',
                'div.article_viewer',
                '.article_viewer',
                '.view_content',
                
                # [ë°±ì—…]
                'div[class*="content"]',
                'article',
                '.article'
            ]
            
            for selector in selectors:
                try:
                    elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    content = elem.text.strip()
                    if len(content) > 100:
                        break
                except:
                    continue
            
            # ì¹˜ìœ ì¼ê¸° ê³ ì • ì•ˆë‚´ë¬¸ ì œê±° (ì˜µì…˜)
            if 'ì¹˜ìœ ì¼ê¸°' in content[:200] and 'ê³ ì •' in content[:200]:
                lines = content.split('\n')
                content = '\n'.join([l for l in lines if 'ê³ ì •' not in l and 'ì•ˆë‚´' not in l])
            
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            view_count = 0
            view_selectors = [
                ".count",
                ".view_count",
                "span.count",
                "span[class*='view']",
                "em.num",
                "*[class*='ViewCount']",
                "*[class*='viewCount']"
            ]
            
            for selector in view_selectors:
                try:
                    elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    text = elem.text.strip()
                    # ìˆ«ìë§Œ ì¶”ì¶œ
                    match = re.search(r'(\d+)', text.replace(',', ''))
                    if match:
                        view_count = int(match.group(1))
                        break
                except:
                    continue
            
            # ì¡°íšŒìˆ˜ë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰
            if view_count == 0:
                try:
                    page_text = self.driver.find_element(By.TAG_NAME, "body").text
                    # "ì¡°íšŒ 123" ë˜ëŠ” "ì¡°íšŒìˆ˜ 123" íŒ¨í„´
                    match = re.search(r'ì¡°íšŒ\s*ìˆ˜?\s*[:\s]*(\d+)', page_text)
                    if match:
                        view_count = int(match.group(1).replace(',', ''))
                except:
                    pass
            
            # ë‚ ì§œ ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€)
            date_text = ''
            date_selectors = [
                ".article_date",
                ".date_time",
                "span.date",
                "div.date",
                "time",
                "*[class*='Date']",
                "*[class*='date']",
                ".writer_info .date",
                ".article_info .date"
            ]
            
            for selector in date_selectors:
                try:
                    elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    text = elem.text.strip()
                    # ë‚ ì§œ íŒ¨í„´ í™•ì¸
                    if text and re.search(r'\d{4}[.-/]\d{2}[.-/]\d{2}|\d{2}[.-/]\d{2}|\d{1,2}:\d{2}', text):
                        date_text = text
                        break
                except:
                    continue
            
            # APIë¡œ ì‘ì„±ì ì •ë³´ ì¶”ì¶œ
            author_info = self.extract_member_id_from_api(clubid, articleid)
            member_id = author_info.get('member_id') if author_info else None
            nickname = author_info.get('nickname') if author_info else 'Unknown'
            
            # ëŒ“ê¸€ ìˆ˜ì§‘ (API ìš°ì„ )
            comments = self.crawl_comments_via_api(clubid, articleid)
            
            return {
                'content': content,
                'date': date_text,
                'viewCount': view_count,
                'member_id': member_id,
                'nickname': nickname,
                'comments': comments,
                'clubid': clubid,
                'articleid': articleid
            }
            
        except Exception as e:
            print(f"[Naver Cafe Crawler] ìƒì„¸ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}", file=sys.stderr)
            return None
    
    def crawl_comments_via_api(self, clubid: str, articleid: str) -> List[Dict]:
        """ëŒ“ê¸€ ìˆ˜ì§‘ (API ìš°íšŒ)"""
        comments = []
        try:
            comment_url = f"https://cafe.naver.com/CommentView.nhn?search.clubid={clubid}&search.articleid={articleid}"
            response = self.session.get(comment_url, timeout=10)
            
            if response.status_code != 200:
                return comments
            
            # JSON ë˜ëŠ” JSONP íŒŒì‹±
            text = response.text
            if text.startswith('callback('):
                text = text[8:-1]  # JSONP ì œê±°
            
            data = json.loads(text)
            
            # ëŒ“ê¸€ í•­ëª© ì¶”ì¶œ (êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
            comment_list = []
            if 'result' in data and 'commentList' in data['result']:
                comment_list = data['result']['commentList']
            elif 'commentList' in data:
                comment_list = data['commentList']
            elif 'comments' in data:
                comment_list = data['comments']
            
            for cmt in comment_list:
                writer_id = (
                    cmt.get('writerId') or
                    cmt.get('memberKey') or
                    cmt.get('userKey') or
                    cmt.get('id') or
                    None
                )
                
                nickname = (
                    cmt.get('nickname') or
                    cmt.get('nickName') or
                    cmt.get('displayName') or
                    'Unknown'
                )
                
                content = cmt.get('content') or cmt.get('text') or ''
                
                if writer_id and content:
                    comments.append({
                        'writer_id': str(writer_id),
                        'nickname': nickname,
                        'content': content
                    })
            
        except Exception as e:
            print(f"[Naver Cafe Crawler] ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}", file=sys.stderr)
        
        return comments
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ (ì•ˆì „í•œ ì¢…ë£Œ)"""
        if self.driver:
            try:
                # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „í™˜
                try:
                    self.driver.switch_to.default_content()
                except:
                    pass
                
                # ë¸Œë¼ìš°ì € ì¢…ë£Œ
                try:
                    self.driver.quit()
                except Exception as e:
                    print(f"[Naver Cafe Crawler] ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}", file=sys.stderr)
                
                # ì¶”ê°€ ì •ë¦¬
                import time
                time.sleep(0.5)
                
            except Exception as e:
                print(f"[Naver Cafe Crawler] ë¸Œë¼ìš°ì € ì¢…ë£Œ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}", file=sys.stderr)
            finally:
                self.driver = None
