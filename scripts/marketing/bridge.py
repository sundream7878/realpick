#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Realpick Marketing Bridge
Next.jsì—ì„œ Python ê¸°ëŠ¥ì„ í˜¸ì¶œí•˜ê¸° ìœ„í•œ ë¸Œë¦¿ì§€ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import io
import json
import argparse
import os
from pathlib import Path

# Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == 'win32':
    sys.stdin = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8')
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# .env íŒŒì¼ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ)
from dotenv import load_dotenv
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env.local íŒŒì¼ ì‚¬ìš©
project_root = Path(__file__).parent.parent.parent
env_path = project_root / '.env.local'
if env_path.exists():
    load_dotenv(dotenv_path=env_path)
else:
    # ë¡œì»¬ .envë„ ì‹œë„
    local_env = Path(__file__).parent / '.env'
    if local_env.exists():
        load_dotenv(dotenv_path=local_env)

from modules.youtube_crawler import YouTubeCrawler
from modules.gemini_analyzer import GeminiAnalyzer
from modules.firebase_manager import FirebaseManager
from modules.email_sender import EmailSender
from modules.community_crawler import CommunityCrawler

# Firebase ì´ˆê¸°í™” (ì „ì—­ìœ¼ë¡œ í•œ ë²ˆë§Œ ì‹¤í–‰, ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
FIREBASE_AVAILABLE = False
try:
    firebase_manager = FirebaseManager()
    if firebase_manager.db:
        FIREBASE_AVAILABLE = True
        print("[Bridge] âœ… Firebase ì´ˆê¸°í™” ì™„ë£Œ", file=sys.stderr)
    else:
        print("[Bridge] âš ï¸ Firebase í‚¤ íŒŒì¼ ì—†ìŒ - ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™”", file=sys.stderr)
except Exception as e:
    print(f"[Bridge] âš ï¸ Firebase ì´ˆê¸°í™” ì‹¤íŒ¨: {e} - ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™”", file=sys.stderr)
try:
    from modules.naver_cafe_crawler import NaverCafeCrawler
    HAS_NAVER_CAFE = True
except ImportError:
    HAS_NAVER_CAFE = False
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    HAS_TRANSCRIPT = True
except ImportError:
    HAS_TRANSCRIPT = False
import google.generativeai as genai
import random
import string
import time
from datetime import timedelta
from firebase_admin import firestore

def crawl_youtube(args):
    """YouTube í¬ë¡¤ë§ (í‚¤ì›Œë“œë¡œ ì˜ìƒ ì§ì ‘ ê²€ìƒ‰)"""
    try:
        api_key = os.getenv('YOUTUBE_API_KEY')
        if not api_key:
            return {
                "success": False,
                "error": "YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        keywords = args.keywords
        max_results = int(getattr(args, 'max_results', 5))
        hours_back = int(getattr(args, 'hours_back', 24))  # ê¸°ë³¸ê°’ 24ì‹œê°„
        
        crawler = YouTubeCrawler(api_key)
        
        # í‚¤ì›Œë“œë¡œ ì˜ìƒ ì§ì ‘ ê²€ìƒ‰ (ì±„ë„ ê²€ìƒ‰ì´ ì•„ë‹˜)
        # ìµœê·¼ 24ì‹œê°„ ì´ë‚´ ì—…ë¡œë“œëœ ì˜ìƒ ì¤‘ ì¡°íšŒìˆ˜ ìƒìœ„ ì˜ìƒ ë°˜í™˜
        videos = crawler.search_videos_by_keyword(keywords, max_results, hours_back=hours_back)
        
        if not videos:
            return {
                "success": False,
                "error": f"'{keywords}' í‚¤ì›Œë“œë¡œ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
            }
        
        return {
            "success": True,
            "count": len(videos),
            "videos": videos
        }
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "trace": traceback.format_exc()
        }

def analyze_video(args):
    """ì˜ìƒ ë¶„ì„ ë° AI ë¯¸ì…˜ ìƒì„±"""
    try:
        video_id = getattr(args, 'video_id', None)
        title = getattr(args, 'title', '')
        desc = getattr(args, 'desc', '')
        
        if not video_id:
            return {
                "success": False,
                "error": "video_idê°€ í•„ìš”í•©ë‹ˆë‹¤."
            }
        
        # Gemini API í‚¤ í™•ì¸
        gemini_key = os.getenv('GEMINI_API_KEY')
        if not gemini_key:
            return {
                "success": False,
                "error": "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        # ìë§‰ ê°€ì ¸ì˜¤ê¸°
        transcript_text = ""
        if HAS_TRANSCRIPT:
            try:
                print(f"DEBUG: Fetching transcript for {video_id}", file=sys.stderr)
                transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko', 'en'])
                transcript_text = " ".join([t['text'] for t in transcript_list])
                print(f"DEBUG: Transcript length: {len(transcript_text)}", file=sys.stderr)
            except Exception as e:
                # ìë§‰ì´ ì—†ì–´ë„ ê³„ì† ì§„í–‰
                print(f"DEBUG: Transcript error: {str(e)}", file=sys.stderr)
                transcript_text = f"ìë§‰ ì—†ìŒ. ì œëª©ê³¼ ì„¤ëª…ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. (ì˜¤ë¥˜: {str(e)})"
        else:
            print("DEBUG: HAS_TRANSCRIPT is False", file=sys.stderr)
            transcript_text = "ìë§‰ APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì œëª©ê³¼ ì„¤ëª…ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."
        
        # Geminië¡œ ë¶„ì„
        print(f"DEBUG: Analyzing with Gemini. Title: {title[:20]}", file=sys.stderr)
        analyzer = GeminiAnalyzer(gemini_key)
        video_info = {
            'title': title.strip('"'),
            'description': desc.strip('"'),
            'video_id': video_id
        }
        
        result = analyzer.analyze_with_transcript(video_info, transcript_text)
        print(f"DEBUG: Gemini result: {str(result)[:100]}", file=sys.stderr)
        
        if result and 'missions' in result:
            return {
                "success": True,
                "missions": result['missions']
            }
        else:
            return {
                "success": False,
                "error": "ë¯¸ì…˜ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Gemini ì‘ë‹µì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def crawl_naver_cafe(args):
    """ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ (Selenium ê¸°ë°˜)"""
    try:
        if not HAS_NAVER_CAFE:
            return {
                "success": False,
                "error": "ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ëŸ¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install selenium undetected-chromedriver"
            }
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        cafe_url = getattr(args, 'cafe_url', None)
        cafe_list_str = getattr(args, 'cafe_list', None)
        keywords_str = getattr(args, 'keywords', None)
        start_date_str = getattr(args, 'start_date', None)
        end_date_str = getattr(args, 'end_date', None)
        exclude_boards = getattr(args, 'exclude_boards', '').split(',') if getattr(args, 'exclude_boards', None) else []
        max_pages = int(getattr(args, 'max_pages', 50))
        use_browser = getattr(args, 'use_browser', 'true').lower() == 'true'
        limit = int(getattr(args, 'limit', 30))
        
        # cafe_listê°€ ìˆìœ¼ë©´ ì—¬ëŸ¬ ì¹´í˜ ìˆœíšŒ
        cafe_urls = []
        if cafe_list_str:
            cafe_urls = [url.strip() for url in cafe_list_str.split(',') if url.strip()]
            print(f"[Naver Cafe Crawl] {len(cafe_urls)}ê°œ ì¹´í˜ ìˆœíšŒ ì˜ˆì •", file=sys.stderr)
        elif cafe_url:
            cafe_urls = [cafe_url]
        
        if not cafe_urls:
            return {"success": False, "error": "cafe_url ë˜ëŠ” cafe_listê°€ í•„ìš”í•©ë‹ˆë‹¤."}
        
        # í‚¤ì›Œë“œ íŒŒì‹±
        keywords = []
        if keywords_str:
            keywords = [kw.strip() for kw in keywords_str.split(',') if kw.strip()]
        
        if not keywords:
            # ê¸°ë³¸ í‚¤ì›Œë“œ (ë¦¬ì–¼í”½ ì£¼ìš” í”„ë¡œê·¸ë¨)
            keywords = ["ë‚˜ëŠ”ì†”ë¡œ", "ë‚˜ì†”", "ìµœê°•ì•¼êµ¬", "ë‚˜ì†”ì‚¬ê³„", "ëŒì‹±ê¸€ì¦ˆ", "í™˜ìŠ¹ì—°ì• ", "ì†”ë¡œì§€ì˜¥"]
        
        # ë‚ ì§œ íŒŒì‹± (ê¸°ë³¸ê°’: ìµœê·¼ 24ì‹œê°„)
        from datetime import datetime
        if start_date_str:
            start_date = datetime.fromisoformat(start_date_str)
        else:
            start_date = datetime.now() - timedelta(hours=24)  # 24ì‹œê°„ ì „
        
        if end_date_str:
            end_date = datetime.fromisoformat(end_date_str)
        else:
            end_date = datetime.now()
        
        # Firestoreì—ì„œ ê¸°ì¡´ ìˆ˜ì§‘ëœ post_id í™•ì¸ (ìŠ¤ë§ˆíŠ¸ ì¬ê°œ)
        existing_post_ids = set()
        try:
            from modules.firebase_manager import FirebaseManager
            fb_manager = FirebaseManager()
            # viral_posts ì»¬ë ‰ì…˜ì—ì„œ í•´ë‹¹ ì¹´í˜ì˜ post_id ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            # ì‹¤ì œ êµ¬í˜„ì€ FirebaseManagerì— ë©”ì„œë“œ ì¶”ê°€ í•„ìš”
        except:
            pass
        
        crawler = NaverCafeCrawler(headless=False, visible=True)
        all_posts = []
        
        if use_browser:
            # ë¸Œë¼ìš°ì € ì‹œì‘
            if not crawler.start_browser():
                return {"success": False, "error": "ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨"}
            
            # [2026-02-10 ìˆ˜ì •] ì¿ í‚¤ ìë™ ë¡œê·¸ì¸ ë¹„í™œì„±í™” â†’ í•­ìƒ ìˆ˜ë™ ë¡œê·¸ì¸
            print("[Naver Cafe Crawl] ============================================", file=sys.stderr)
            print("[Naver Cafe Crawl] ğŸ” ìˆ˜ë™ ë¡œê·¸ì¸ ëª¨ë“œ", file=sys.stderr)
            print("[Naver Cafe Crawl] ============================================", file=sys.stderr)
            print("[Naver Cafe Crawl]", file=sys.stderr)
            print("[Naver Cafe Crawl] ğŸ“Œ Chrome ë¸Œë¼ìš°ì €ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤.", file=sys.stderr)
            print("[Naver Cafe Crawl] ğŸ“Œ ë„¤ì´ë²„ ë¡œê·¸ì¸ í˜ì´ì§€ì—ì„œ ì§ì ‘ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.", file=sys.stderr)
            print("[Naver Cafe Crawl] ğŸ“Œ ë¡œê·¸ì¸ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ í¬ë¡¤ë§ì´ ì‹œì‘ë©ë‹ˆë‹¤.", file=sys.stderr)
            print("[Naver Cafe Crawl]", file=sys.stderr)
            print("[Naver Cafe Crawl] â³ ë¡œê·¸ì¸ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘... (ìµœëŒ€ 5ë¶„)", file=sys.stderr)
            print("[Naver Cafe Crawl] ============================================", file=sys.stderr)
            
            # ìˆ˜ë™ ë¡œê·¸ì¸ ëŒ€ê¸° (ì¿ í‚¤ ì €ì¥ ì•ˆ í•¨)
            if not crawler.wait_for_login(timeout=300, save_cookies=False):
                crawler.close()
                return {"success": False, "error": "ë¡œê·¸ì¸ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ (5ë¶„)"}
            
            print("[Naver Cafe Crawl] âœ… ë¡œê·¸ì¸ ì™„ë£Œ! í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.", file=sys.stderr)
        
        # Firestore Progress ì—…ë°ì´íŠ¸ í•¨ìˆ˜ (Firebase ì‚¬ìš© ê°€ëŠ¥ ì‹œì—ë§Œ)
        progress_id = getattr(args, 'progress_id', None) or f"naver_cafe_{int(time.time())}"
        
        def update_progress(current, total, message, status="running", add_log=None):
            if not FIREBASE_AVAILABLE:
                # Firebase ì—†ìœ¼ë©´ ì½˜ì†”ì—ë§Œ ì¶œë ¥
                print(f"[Progress] {status}: {message} ({current}/{total})", file=sys.stderr)
                return
            try:
                db = firestore.client()
                doc_ref = db.collection('crawl_progress').document(progress_id)
                
                update_data = {
                    'current': current,
                    'total': total,
                    'message': message,
                    'status': status,
                    'updatedAt': firestore.SERVER_TIMESTAMP
                }
                
                # ë¡œê·¸ ì¶”ê°€ (ë°°ì—´ì— append)
                if add_log:
                    from datetime import datetime
                    log_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'message': add_log
                    }
                    update_data['logs'] = firestore.ArrayUnion([log_entry])
                
                doc_ref.set(update_data, merge=True)
            except Exception as e:
                print(f"[Progress Update Error] {e}", file=sys.stderr)
        
        # ì—¬ëŸ¬ ì¹´í˜ë¥¼ ìˆœíšŒí•˜ë©´ì„œ limit ê°œìˆ˜ë§Œí¼ ìˆ˜ì§‘
        collected_count = 0
        update_progress(0, limit, "ğŸŒ Chrome ë¸Œë¼ìš°ì €ë¡œ ì¹´í˜ ì ‘ì† ì¤‘...", "running", add_log="ğŸš€ í¬ë¡¤ë§ ì‹œì‘")
        
        print(f"[Naver Cafe Crawl] ============================================", file=sys.stderr)
        print(f"[Naver Cafe Crawl] ğŸ¯ ëª©í‘œ: {limit}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘", file=sys.stderr)
        print(f"[Naver Cafe Crawl] ğŸ“‚ ìˆœíšŒí•  ì¹´í˜: {len(cafe_urls)}ê°œ", file=sys.stderr)
        print(f"[Naver Cafe Crawl] â° ë‚ ì§œ ë²”ìœ„: ìµœê·¼ 24ì‹œê°„", file=sys.stderr)
        print(f"[Naver Cafe Crawl] ============================================", file=sys.stderr)
        
        for cafe_idx, cafe_url in enumerate(cafe_urls):
            if collected_count >= limit:
                print(f"\n[Naver Cafe Crawl] âœ… ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±: {collected_count}/{limit}ê°œ", file=sys.stderr)
                update_progress(collected_count, limit, f"âœ… ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±: {collected_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ", "completed")
                break
            
            remaining = limit - collected_count
            print(f"\n[Naver Cafe Crawl] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", file=sys.stderr)
            print(f"[Naver Cafe Crawl] ğŸ“ ì¹´í˜ [{cafe_idx+1}/{len(cafe_urls)}]: {cafe_url}", file=sys.stderr)
            print(f"[Naver Cafe Crawl] ğŸ¯ í˜„ì¬: {collected_count}ê°œ / ëª©í‘œ: {limit}ê°œ (ë‚¨ì€ ê°œìˆ˜: {remaining}ê°œ)", file=sys.stderr)
            print(f"[Naver Cafe Crawl] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", file=sys.stderr)
            
            update_progress(collected_count, limit, f"ğŸ“‚ [{cafe_idx+1}/{len(cafe_urls)}] ì¹´í˜ í¬ë¡¤ë§ ì¤‘... ({collected_count}/{limit}ê°œ ìˆ˜ì§‘)", "running", add_log=f"ğŸ“‚ ì¹´í˜ [{cafe_idx+1}/{len(cafe_urls)}] ì ‘ì† ì¤‘ (ë‚¨ì€ ê°œìˆ˜: {remaining}ê°œ)")
            
            # ì¹´í˜ í¬ë¡¤ë§ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            cafe_start_time = time.time()
            
            # ëª©ë¡ ìˆ˜ì§‘ (í‚¤ì›Œë“œ ê¸°ë°˜)
            print(f"[Naver Cafe Crawl] ğŸš€ Gemini 3ë‹¨ê³„ ì „ëµ ì ìš©:", file=sys.stderr)
            print(f"[Naver Cafe Crawl]   1ï¸âƒ£ ëª©ë¡ ë·° ê°•ì œ (viewType=L)", file=sys.stderr)
            print(f"[Naver Cafe Crawl]   2ï¸âƒ£ ì •ê·œì‹ ë‚ ì§œ íŒ¨í„´ (9ê°€ì§€)", file=sys.stderr)
            print(f"[Naver Cafe Crawl]   3ï¸âƒ£ ìƒì„¸ í˜ì´ì§€ Fallback (ìµœì´ˆ 10ê°œ)", file=sys.stderr)
            posts_list = crawler.crawl_article_list(
                cafe_url=cafe_url,
                keywords=keywords,
                start_date=start_date,
                end_date=end_date,
                exclude_boards=exclude_boards,
                max_pages=max_pages
            )
            
            print(f"[Naver Cafe Crawl] ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ: {len(posts_list)}ê°œ", file=sys.stderr)
            
            if len(posts_list) == 0:
                print(f"[Naver Cafe Crawl] âš ï¸ ì´ ì¹´í˜ì—ì„œ ê²Œì‹œê¸€ ì—†ìŒ. ë‹¤ìŒ ì¹´í˜ë¡œ ì¦‰ì‹œ ì´ë™... (ë”œë ˆì´ ì—†ìŒ)", file=sys.stderr)
                update_progress(collected_count, limit, f"âš ï¸ ê²Œì‹œê¸€ ì—†ìŒ, ë‹¤ìŒ ì¹´í˜ë¡œ ì´ë™ ì¤‘...", "running", add_log=f"âš ï¸ ì¹´í˜ [{cafe_idx+1}] ê²Œì‹œê¸€ ì—†ìŒ")
                # ê²Œì‹œê¸€ì´ ì—†ìœ¼ë©´ ëŒ€ê¸° ì—†ì´ ì¦‰ì‹œ ë‹¤ìŒ ì¹´í˜ë¡œ
                continue
            
            update_progress(collected_count, limit, f"ğŸ“ ê²Œì‹œê¸€ {len(posts_list)}ê°œ ë°œê²¬, ìƒì„¸ ìˆ˜ì§‘ ì¤‘...", "running", add_log=f"ğŸ“ ì¹´í˜ [{cafe_idx+1}] ê²Œì‹œê¸€ {len(posts_list)}ê°œ ë°œê²¬")
            
            # ìƒì„¸ ìˆ˜ì§‘ (limit ê°œìˆ˜ê¹Œì§€ë§Œ)
            for idx, post_info in enumerate(posts_list):
                if collected_count >= limit:
                    break
                
                post_id = post_info.get('post_id')
                
                # ìŠ¤ë§ˆíŠ¸ ì¬ê°œ: ì´ë¯¸ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì€ ìŠ¤í‚µ
                if post_id in existing_post_ids:
                    print(f"[Naver Cafe Crawl] ìŠ¤í‚µ (ì´ë¯¸ ìˆ˜ì§‘ë¨): {post_id}", file=sys.stderr)
                    continue
                
                url = post_info.get('url')
                if not url:
                    continue
                
                # 10ê°œë§ˆë‹¤ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
                if idx > 0 and idx % 10 == 0:
                    print(f"[Naver Cafe Crawl] ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì¤‘... ({idx}ë²ˆì§¸ ê²Œì‹œê¸€)", file=sys.stderr)
                    update_progress(collected_count, limit, f"ğŸ” ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì¤‘...", "running", add_log=f"ğŸ” ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ({idx}ë²ˆì§¸)")
                    if not crawler.check_login_status():
                        print(f"[Naver Cafe Crawl] âš ï¸ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.", file=sys.stderr)
                        update_progress(collected_count, limit, f"âš ï¸ ë¡œê·¸ì¸ í•„ìš”, ë‹¤ì‹œ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”", "running", add_log=f"âš ï¸ ë¡œê·¸ì¸ ì„¸ì…˜ ë§Œë£Œ, ì¬ë¡œê·¸ì¸ í•„ìš”")
                        if not crawler.wait_for_login(timeout=300, save_cookies=False):
                            print(f"[Naver Cafe Crawl] âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.", file=sys.stderr)
                            update_progress(collected_count, limit, f"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨", "failed", add_log=f"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨ë¡œ ì¤‘ë‹¨")
                            break
                
                # ìƒì„¸ ìˆ˜ì§‘
                detail = crawler.crawl_article_detail(url, post_id)
                if detail:
                    # ëª©ë¡ ì •ë³´ì™€ ìƒì„¸ ì •ë³´ ë³‘í•©
                    merged_post = {
                        **post_info,
                        'content': detail.get('content', ''),
                        'member_id': detail.get('member_id') or post_info.get('member_id'),
                        'nickname': detail.get('nickname') or post_info.get('nickname'),
                        'comments': detail.get('comments', []),
                        'viewCount': 0,  # í•„ìš”ì‹œ ì¶”ê°€
                        'commentCount': len(detail.get('comments', []))
                    }
                    all_posts.append(merged_post)
                    existing_post_ids.add(post_id)
                    collected_count += 1
                    post_title = merged_post.get('title', 'ì œëª© ì—†ìŒ')[:30]
                    update_progress(collected_count, limit, f"ğŸ“ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘: {collected_count}/{limit}ê°œ (ì¹´í˜ {cafe_idx+1}/{len(cafe_urls)})", "running", add_log=f"âœ… ìˆ˜ì§‘: {post_title}... ({collected_count}/{limit})")
                    print(f"[Naver Cafe Crawl] âœ… ìˆ˜ì§‘ ì§„í–‰: {collected_count}/{limit}ê°œ", file=sys.stderr)
                    
                    # Rate limiting (ì„±ê³µ ì‹œì—ë§Œ ê¸´ ë”œë ˆì´)
                    time.sleep(random.uniform(2, 4))
                else:
                    # ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ì§§ì€ ë”œë ˆì´
                    time.sleep(0.5)
        
            # ì¹´í˜ë³„ ìˆ˜ì§‘ ê²°ê³¼ ì²´í¬
            cafe_collected = len([p for p in all_posts if p.get('cafe_url') == cafe_url])
            cafe_elapsed = time.time() - cafe_start_time
            
            print(f"\n[Naver Cafe Crawl] ğŸ“Š ì¹´í˜ [{cafe_idx+1}/{len(cafe_urls)}] ê²°ê³¼:", file=sys.stderr)
            if cafe_collected == 0:
                print(f"[Naver Cafe Crawl]   âš ï¸ ìˆ˜ì§‘ ì‹¤íŒ¨: 0ê°œ (ì†Œìš”: {cafe_elapsed:.1f}ì´ˆ)", file=sys.stderr)
                print(f"[Naver Cafe Crawl]   ğŸš€ ë‹¤ìŒ ì¹´í˜ë¡œ ì¦‰ì‹œ ì´ë™...", file=sys.stderr)
                update_progress(collected_count, limit, f"ğŸš€ ë‹¤ìŒ ì¹´í˜ë¡œ ì´ë™ ì¤‘...", "running", add_log=f"ğŸ“Š ì¹´í˜ [{cafe_idx+1}] ì™„ë£Œ: 0ê°œ")
                # ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ëŒ€ê¸° ì—†ì´ ë‹¤ìŒ ì¹´í˜ë¡œ
            else:
                print(f"[Naver Cafe Crawl]   âœ… ìˆ˜ì§‘ ì„±ê³µ: {cafe_collected}ê°œ (ì†Œìš”: {cafe_elapsed:.1f}ì´ˆ)", file=sys.stderr)
                print(f"[Naver Cafe Crawl]   ğŸ“ˆ ëˆ„ì : {collected_count}/{limit}ê°œ", file=sys.stderr)
                update_progress(collected_count, limit, f"ğŸ“Š ëˆ„ì : {collected_count}/{limit}ê°œ", "running", add_log=f"ğŸ“Š ì¹´í˜ [{cafe_idx+1}] ì™„ë£Œ: {cafe_collected}ê°œ ìˆ˜ì§‘ ({cafe_elapsed:.1f}ì´ˆ)")
                # ìˆ˜ì§‘ ì„±ê³µ ì‹œ ë‹¤ìŒ ì¹´í˜ë¡œ ë„˜ì–´ê°€ê¸° ì „ ì§§ì€ ëŒ€ê¸°
                time.sleep(1)
            
            # ì•„ì§ ëª©í‘œì— ë„ë‹¬í•˜ì§€ ëª»í–ˆìœ¼ë©´ ê³„ì† ì§„í–‰
            if collected_count < limit and cafe_idx < len(cafe_urls) - 1:
                print(f"[Naver Cafe Crawl] ğŸ”„ ëª©í‘œ ë¯¸ë‹¬ì„± â†’ ë‹¤ìŒ ì¹´í˜ë¡œ ê³„ì†...", file=sys.stderr)
        
        # í¬ë¡¤ë§ ì™„ë£Œ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        print(f"\n[Naver Cafe Crawl] ============================================", file=sys.stderr)
        print(f"[Naver Cafe Crawl] ğŸ í¬ë¡¤ë§ ì™„ë£Œ", file=sys.stderr)
        print(f"[Naver Cafe Crawl] ğŸ“Š ìµœì¢… ê²°ê³¼: {collected_count}/{limit}ê°œ ìˆ˜ì§‘", file=sys.stderr)
        print(f"[Naver Cafe Crawl] ğŸ“‚ ìˆœíšŒí•œ ì¹´í˜: {min(cafe_idx + 1, len(cafe_urls))}/{len(cafe_urls)}ê°œ", file=sys.stderr)
        print(f"[Naver Cafe Crawl] ============================================", file=sys.stderr)
        
        if collected_count >= limit:
            update_progress(collected_count, limit, f"âœ… ëª©í‘œ ë‹¬ì„±: {collected_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ", "completed", add_log=f"ğŸ í¬ë¡¤ë§ ì™„ë£Œ: {collected_count}ê°œ ìˆ˜ì§‘ ì„±ê³µ")
        else:
            update_progress(collected_count, limit, f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬: {collected_count}/{limit}ê°œ ìˆ˜ì§‘ (ëª¨ë“  ì¹´í˜ ìˆœíšŒ ì™„ë£Œ)", "completed", add_log=f"ğŸ í¬ë¡¤ë§ ì™„ë£Œ: {collected_count}/{limit}ê°œ ìˆ˜ì§‘")
        
        return {
            "success": True,
            "posts": all_posts,
            "total": len(all_posts)
        }
        
    except Exception as e:
        import traceback
        print(f"[Naver Cafe Crawl] ì˜¤ë¥˜: {e}", file=sys.stderr)
        print(traceback.format_exc(), file=sys.stderr)
        
        # ì‹¤íŒ¨ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (Firebase ì‚¬ìš© ê°€ëŠ¥ ì‹œì—ë§Œ)
        if FIREBASE_AVAILABLE:
            try:
                db = firestore.client()
                progress_id = getattr(args, 'progress_id', None) or f"naver_cafe_{int(time.time())}"
                db.collection('crawl_progress').document(progress_id).set({
                    'status': 'failed',
                    'message': f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                    'updatedAt': firestore.SERVER_TIMESTAMP
                }, merge=True)
            except:
                pass
        
        return {
            "success": False,
            "error": str(e)
        }
    finally:
        # ë¸Œë¼ìš°ì € ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
        if use_browser and crawler:
            try:
                crawler.close()
            except Exception as e:
                print(f"[Naver Cafe Crawl] ë¸Œë¼ìš°ì € ì¢…ë£Œ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}", file=sys.stderr)

def crawl_community(args):
    """ì»¤ë®¤ë‹ˆí‹° ì´ìŠˆ í¬ë¡¤ë§ ë° ëŒ“ê¸€ ìƒì„±"""
    try:
        gemini_key = os.getenv('GEMINI_API_KEY')
        if not gemini_key:
            return {"success": False, "error": "Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
            
        crawler = CommunityCrawler()
        analyzer = GeminiAnalyzer(gemini_key)
        
        # íŒŒë¼ë¯¸í„° ë°›ê¸°
        limit = int(getattr(args, 'limit', 30))
        selected_show_ids = getattr(args, 'selected_show_ids', '')
        start_date_str = getattr(args, 'start_date', None)
        end_date_str = getattr(args, 'end_date', None)
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        from datetime import datetime
        if start_date_str:
            start_date = datetime.fromisoformat(start_date_str)
        else:
            start_date = datetime.now() - timedelta(days=1)  # ê¸°ë³¸ê°’: 1ì¼ ì „
        
        if end_date_str:
            end_date = datetime.fromisoformat(end_date_str)
        else:
            end_date = datetime.now()
        
        print(f"[Community Crawl] ë‚ ì§œ ë²”ìœ„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}", file=sys.stderr)
        
        # ì „ì²´ í”„ë¡œê·¸ë¨ í‚¤ì›Œë“œ ë§µ
        all_program_keywords = {
            "nasolo": {"showId": "nasolo", "keywords": ["ë‚˜ëŠ”ì†”ë¡œ", "ë‚˜ì†”", "ë‚¨ê·œí™"]},
            "choegang-yagu-2025": {"showId": "choegang-yagu-2025", "keywords": ["ìµœê°•ì•¼êµ¬", "ëª¬ìŠ¤í„°ì¦ˆ", "ê¹€ì„±ê·¼"]},
            "nasolsagye": {"showId": "nasolsagye", "keywords": ["ë‚˜ì†”ì‚¬ê³„", "ì‚¬ë‘ì€ ê³„ì†ëœë‹¤"]},
            "dolsingles6": {"showId": "dolsingles6", "keywords": ["ëŒì‹±ê¸€ì¦ˆ", "ëŒì‹±"]},
            "hwanseung4": {"showId": "hwanseung4", "keywords": ["í™˜ìŠ¹ì—°ì• ", "í™˜ì—°"]},
            "solojihuk5": {"showId": "solojihuk5", "keywords": ["ì†”ë¡œì§€ì˜¥"]},
            "culinary-class-wars2": {"showId": "culinary-class-wars2", "keywords": ["í‘ë°±ìš”ë¦¬ì‚¬", "ì•ˆì„±ì¬", "ë°±ì¢…ì›"]},
            "goal-girls-8": {"showId": "goal-girls-8", "keywords": ["ê³¨ë•Œë…€", "ê³¨ ë•Œë¦¬ëŠ” ê·¸ë…€ë“¤"]}
        }
        
        # ì„ íƒëœ í”„ë¡œê·¸ë¨ë§Œ í•„í„°ë§
        if selected_show_ids:
            selected_ids = [sid.strip() for sid in selected_show_ids.split(',') if sid.strip()]
            program_keywords = [all_program_keywords[sid] for sid in selected_ids if sid in all_program_keywords]
            print(f"[Community Crawl] ì„ íƒëœ í”„ë¡œê·¸ë¨: {len(program_keywords)}ê°œ", file=sys.stderr)
        else:
            # ê¸°ë³¸ê°’: ëª¨ë“  í”„ë¡œê·¸ë¨
            program_keywords = list(all_program_keywords.values())
            print(f"[Community Crawl] ì „ì²´ í”„ë¡œê·¸ë¨ í¬ë¡¤ë§: {len(program_keywords)}ê°œ", file=sys.stderr)
        
        all_results = []
        # limitì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨í•˜ê¸° ìœ„í•œ í”Œë˜ê·¸
        reached_limit = False
        
        # ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ ëœë¤í•˜ê²Œ ëª‡ ê°œì˜ í”„ë¡œê·¸ë¨ë§Œ ì„ íƒí•˜ê±°ë‚˜ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
        for prog in program_keywords:
            if reached_limit:
                break
                
            show_id = prog["showId"]
            kws = prog["keywords"]
            
            # ê° í”„ë¡œê·¸ë¨ë³„ í‚¤ì›Œë“œë¡œ 10ëŒ€ ì»¤ë®¤ë‹ˆí‹° ëª¨ë‹ˆí„°ë§
            # limitì„ ì „ë‹¬í•˜ì—¬ í¬ë¡¤ë§ ê°œìˆ˜ ì œí•œ
            remaining_limit = limit - len(all_results)
            if remaining_limit <= 0:
                reached_limit = True
                break
                
            posts = crawler.get_hot_posts(show_id, kws, limit=remaining_limit)
            
            for post in posts:
                # limitì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                if len(all_results) >= limit:
                    reached_limit = True
                    break
                    
                # AI ëŒ“ê¸€ ìƒì„±
                comment = analyzer.generate_viral_comment(post.get('content', ''), post.get('title', ''))
                post['suggestedComment'] = comment
                all_results.append(post)
                
        # ì¡°íšŒìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        all_results.sort(key=lambda x: x.get('viewCount', 0), reverse=True)
        
        return {
            "success": True,
            "posts": all_results[:limit] # limitë§Œí¼ë§Œ ë°˜í™˜
        }
    except Exception as e:
        return {"success": False, "error": str(e)}

def main():
    # ëª¨ë“  ê²½ê³  ë©”ì‹œì§€ë¥¼ ë¬´ì‹œí•˜ì—¬ JSON ì¶œë ¥ë§Œ ê¹¨ë—í•˜ê²Œ ìœ ì§€
    import warnings
    warnings.filterwarnings("ignore")
    
    # ëª¨ë“  printë¥¼ stderrë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (JSON ì¶œë ¥ë§Œ stdoutì—)
    original_stdout = sys.stdout
    sys.stdout = sys.stderr
    
    try:
        parser = argparse.ArgumentParser(description='Realpick Marketing Bridge')
        parser.add_argument('--args-file', type=str, help='JSON file with arguments')
        parser.add_argument('command', type=str, nargs='?', help='Command to execute')
        
        # ê³µí†µ ì¸ì
        parser.add_argument('--keywords', type=str, help='Search keywords')
        parser.add_argument('--max-results', type=int, help='Maximum results')
        parser.add_argument('--video-id', type=str, help='YouTube video ID')
        parser.add_argument('--title', type=str, help='Video title')
        parser.add_argument('--desc', type=str, help='Video description')
        
        args, _ = parser.parse_known_args()
        
        # --args-fileì´ ì œê³µë˜ë©´ JSON íŒŒì¼ì—ì„œ ì¸ì ë¡œë“œ
        if args.args_file and os.path.exists(args.args_file):
            with open(args.args_file, 'r', encoding='utf-8') as f:
                json_args = json.load(f)
            
            # JSON ì¸ìë¥¼ argparse.Namespaceë¡œ ë³€í™˜
            for key, value in json_args.items():
                if key == 'command':
                    args.command = value
                else:
                    setattr(args, key.replace('-', '_'), value)
        
        # ëª…ë ¹ì–´ì— ë”°ë¼ í•¨ìˆ˜ ì‹¤í–‰
        result = None
        if args.command == 'crawl-youtube':
            result = crawl_youtube(args)
        elif args.command == 'analyze-video':
            result = analyze_video(args)
        elif args.command == 'crawl-community':
            result = crawl_community(args)
        elif args.command == 'crawl-naver-cafe':
            result = crawl_naver_cafe(args)
        else:
            result = {
                "success": False,
                "error": f"Unknown command: {args.command}"
            }
        
        # JSON ì¶œë ¥ (ë°˜ë“œì‹œ stdoutì— í•œ ì¤„ë¡œ)
        sys.stdout = original_stdout
        print(json.dumps(result, ensure_ascii=False))
        sys.stdout.flush()
        
    except Exception as e:
        import traceback
        # ì˜¤ë¥˜ë„ stdoutìœ¼ë¡œ JSON í˜•ì‹ìœ¼ë¡œ
        sys.stdout = original_stdout
        print(json.dumps({
            "success": False, 
            "error": str(e),
            "trace": traceback.format_exc()
        }, ensure_ascii=False))
        sys.stdout.flush()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        print(json.dumps({"success": False, "error": str(e)}, ensure_ascii=False))
        sys.stdout.flush()
        sys.exit(1)
