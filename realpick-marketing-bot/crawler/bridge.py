#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Realpick Marketing Bridge
Next.jsì—ì„œ Python ê¸°ëŠ¥ì„ í˜¸ì¶œí•˜ê¸° ìœ„í•œ ë¸Œë¦¿ì§€ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import io
import json
import argparse
import os
from pathlib import Path

# Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == 'win32':
    sys.stdin = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8')
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# .env íŒŒì¼ ë¡œë“œ (realpick-marketing-bot ë£¨íŠ¸ì—ì„œ)
from dotenv import load_dotenv
# realpick-marketing-bot/.env.local íŒŒì¼ ì‚¬ìš©
bot_root = Path(__file__).parent.parent  # realpick-marketing-bot/
env_path = bot_root / '.env.local'
if env_path.exists():
    load_dotenv(dotenv_path=env_path)
    print(f"[Bridge] .env.local ë¡œë“œë¨: {env_path}", file=sys.stderr)
else:
    # ë¡œì»¬ .envë„ ì‹œë„
    local_env = Path(__file__).parent / '.env'
    if local_env.exists():
        load_dotenv(dotenv_path=local_env)
    else:
        print(f"[Bridge] ê²½ê³ : .env.local íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_path}", file=sys.stderr)

from modules.youtube_crawler import YouTubeCrawler
from modules.gemini_analyzer import GeminiAnalyzer
from modules.firebase_manager import FirebaseManager
from modules.email_sender import EmailSender
from modules.community_crawler import CommunityCrawler
try:
    from modules.naver_cafe_crawler import NaverCafeCrawler
    HAS_NAVER_CAFE = True
except ImportError:
    HAS_NAVER_CAFE = False
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    HAS_TRANSCRIPT = True
except ImportError:
    HAS_TRANSCRIPT = False
import google.generativeai as genai
import random
import string
import time
from datetime import timedelta
from firebase_admin import firestore

def crawl_youtube(args):
    """YouTube í¬ë¡¤ë§ (í‚¤ì›Œë“œë¡œ ì˜ìƒ ì§ì ‘ ê²€ìƒ‰)"""
    try:
        api_key = os.getenv('YOUTUBE_API_KEY')
        if not api_key:
            return {
                "success": False,
                "error": "YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        keywords = args.keywords
        max_results = int(getattr(args, 'max_results', 5))
        hours_back = int(getattr(args, 'hours_back', 24))  # ê¸°ë³¸ê°’ 24ì‹œê°„
        
        crawler = YouTubeCrawler(api_key)
        
        # í‚¤ì›Œë“œë¡œ ì˜ìƒ ì§ì ‘ ê²€ìƒ‰ (ì±„ë„ ê²€ìƒ‰ì´ ì•„ë‹˜)
        # ìµœê·¼ 24ì‹œê°„ ì´ë‚´ ì—…ë¡œë“œëœ ì˜ìƒ ì¤‘ ì¡°íšŒìˆ˜ ìƒìœ„ ì˜ìƒ ë°˜í™˜
        videos = crawler.search_videos_by_keyword(keywords, max_results, hours_back=hours_back)
        
        if not videos:
            return {
                "success": False,
                "error": f"'{keywords}' í‚¤ì›Œë“œë¡œ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
            }
        
        return {
            "success": True,
            "count": len(videos),
            "videos": videos
        }
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "trace": traceback.format_exc()
        }

def analyze_video(args):
    """ì˜ìƒ ë¶„ì„ ë° AI ë¯¸ì…˜ ìƒì„±"""
    try:
        video_id = getattr(args, 'video_id', None)
        title = getattr(args, 'title', '')
        desc = getattr(args, 'desc', '')
        
        if not video_id:
            return {
                "success": False,
                "error": "video_idê°€ í•„ìš”í•©ë‹ˆë‹¤."
            }
        
        # Gemini API í‚¤ í™•ì¸
        gemini_key = os.getenv('GEMINI_API_KEY')
        if not gemini_key:
            return {
                "success": False,
                "error": "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        # ìë§‰ ê°€ì ¸ì˜¤ê¸°
        transcript_text = ""
        if HAS_TRANSCRIPT:
            try:
                print(f"DEBUG: Fetching transcript for {video_id}", file=sys.stderr)
                transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko', 'en'])
                transcript_text = " ".join([t['text'] for t in transcript_list])
                print(f"DEBUG: Transcript length: {len(transcript_text)}", file=sys.stderr)
            except Exception as e:
                # ìë§‰ì´ ì—†ì–´ë„ ê³„ì† ì§„í–‰
                print(f"DEBUG: Transcript error: {str(e)}", file=sys.stderr)
                transcript_text = f"ìë§‰ ì—†ìŒ. ì œëª©ê³¼ ì„¤ëª…ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. (ì˜¤ë¥˜: {str(e)})"
        else:
            print("DEBUG: HAS_TRANSCRIPT is False", file=sys.stderr)
            transcript_text = "ìë§‰ APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì œëª©ê³¼ ì„¤ëª…ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."
        
        # Geminië¡œ ë¶„ì„ (í¬ë¡¤ë§ ì‹œ ì„ íƒí•œ í”„ë¡œê·¸ë¨ í‚¤ì›Œë“œ ì „ë‹¬ â†’ ìë§‰ê³¼ ë§ëŠ” í”„ë¡œê·¸ë¨ ë¶„ë¥˜ìš©)
        keyword = getattr(args, 'keyword', '') or ''
        if isinstance(keyword, str):
            keyword = keyword.strip().strip('"')
        print(f"DEBUG: Analyzing with Gemini. Title: {title[:20]}, keyword: {keyword}", file=sys.stderr)
        analyzer = GeminiAnalyzer(gemini_key)
        video_info = {
            'title': title.strip('"'),
            'description': desc.strip('"'),
            'video_id': video_id,
            'keyword': keyword
        }
        
        result = analyzer.analyze_with_transcript(video_info, transcript_text)
        print(f"DEBUG: Gemini result: {str(result)[:100]}", file=sys.stderr)
        
        if result and 'missions' in result:
            return {
                "success": True,
                "missions": result['missions']
            }
        else:
            return {
                "success": False,
                "error": "ë¯¸ì…˜ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Gemini ì‘ë‹µì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def crawl_naver_cafe(args):
    """ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ (Selenium ê¸°ë°˜)"""
    try:
        if not HAS_NAVER_CAFE:
            return {
                "success": False,
                "error": "ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ëŸ¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install selenium undetected-chromedriver"
            }
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        cafe_url = getattr(args, 'cafe_url', None)
        keywords_str = getattr(args, 'keywords', None)
        start_date_str = getattr(args, 'start_date', None)
        end_date_str = getattr(args, 'end_date', None)
        exclude_boards = getattr(args, 'exclude_boards', '').split(',') if getattr(args, 'exclude_boards', None) else []
        max_pages = int(getattr(args, 'max_pages', 50))
        use_browser = getattr(args, 'use_browser', 'true').lower() == 'true'
        
        if not cafe_url:
            return {"success": False, "error": "cafe_urlì´ í•„ìš”í•©ë‹ˆë‹¤."}
        
        # í‚¤ì›Œë“œ íŒŒì‹±
        keywords = []
        if keywords_str:
            keywords = [kw.strip() for kw in keywords_str.split(',') if kw.strip()]
        
        if not keywords:
            # ê¸°ë³¸ í‚¤ì›Œë“œ (ë¦¬ì–¼í”½ ì£¼ìš” í”„ë¡œê·¸ë¨)
            keywords = ["ë‚˜ëŠ”ì†”ë¡œ", "ë‚˜ì†”", "ìµœê°•ì•¼êµ¬", "ë‚˜ì†”ì‚¬ê³„", "ëŒì‹±ê¸€ì¦ˆ", "í™˜ìŠ¹ì—°ì• ", "ì†”ë¡œì§€ì˜¥"]
        
        # ë‚ ì§œ íŒŒì‹± (ê¸°ë³¸ê°’: ìµœê·¼ 24ì‹œê°„)
        from datetime import datetime
        if start_date_str:
            start_date = datetime.fromisoformat(start_date_str)
        else:
            start_date = datetime.now() - timedelta(hours=24)  # 24ì‹œê°„ ì „
        
        if end_date_str:
            end_date = datetime.fromisoformat(end_date_str)
        else:
            end_date = datetime.now()
        
        # Firestoreì—ì„œ ê¸°ì¡´ ìˆ˜ì§‘ëœ post_id í™•ì¸ (ìŠ¤ë§ˆíŠ¸ ì¬ê°œ)
        existing_post_ids = set()
        try:
            from modules.firebase_manager import FirebaseManager
            fb_manager = FirebaseManager()
            # viral_posts ì»¬ë ‰ì…˜ì—ì„œ í•´ë‹¹ ì¹´í˜ì˜ post_id ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            # ì‹¤ì œ êµ¬í˜„ì€ FirebaseManagerì— ë©”ì„œë“œ ì¶”ê°€ í•„ìš”
        except:
            pass
        
        crawler = NaverCafeCrawler(headless=False, visible=True)
        
        if use_browser:
            # ë¸Œë¼ìš°ì € ì‹œì‘
            if not crawler.start_browser():
                return {"success": False, "error": "ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨"}
            
            # [ìŠ¤ë§ˆíŠ¸ ë¡œê·¸ì¸] ì €ì¥ëœ ì¿ í‚¤ë¡œ ìë™ ë¡œê·¸ì¸ ì‹œë„
            print("[Naver Cafe Crawl] ì €ì¥ëœ ì¿ í‚¤ë¡œ ìë™ ë¡œê·¸ì¸ ì‹œë„ ì¤‘...", file=sys.stderr)
            cookie_loaded = crawler.load_login_cookies()
            
            if not cookie_loaded:
                # ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ ë¡œê·¸ì¸
                print("[Naver Cafe Crawl] ì¿ í‚¤ ì—†ìŒ - ìˆ˜ë™ ë¡œê·¸ì¸ í•„ìš”", file=sys.stderr)
                if not crawler.wait_for_login(timeout=300, save_cookies=True):
                    crawler.close()
                    return {"success": False, "error": "ë¡œê·¸ì¸ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼"}
            else:
                print("[Naver Cafe Crawl] âœ… ì¿ í‚¤ë¡œ ìë™ ë¡œê·¸ì¸ ì„±ê³µ", file=sys.stderr)
        
        # ëª©ë¡ ìˆ˜ì§‘ (í‚¤ì›Œë“œ ê¸°ë°˜)
        print(f"[Naver Cafe Crawl] ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘: {cafe_url}, í‚¤ì›Œë“œ: {keywords}", file=sys.stderr)
        print(f"[Naver Cafe Crawl] ğŸš€ Gemini 3ë‹¨ê³„ ì „ëµ ì ìš©:", file=sys.stderr)
        print(f"[Naver Cafe Crawl]   1ï¸âƒ£ ëª©ë¡ ë·° ê°•ì œ (viewType=L)", file=sys.stderr)
        print(f"[Naver Cafe Crawl]   2ï¸âƒ£ ì •ê·œì‹ ë‚ ì§œ íŒ¨í„´ (9ê°€ì§€)", file=sys.stderr)
        print(f"[Naver Cafe Crawl]   3ï¸âƒ£ ìƒì„¸ í˜ì´ì§€ Fallback (ìµœì´ˆ 10ê°œ)", file=sys.stderr)
        posts_list = crawler.crawl_article_list(
            cafe_url=cafe_url,
            keywords=keywords,
            start_date=start_date,
            end_date=end_date,
            exclude_boards=exclude_boards,
            max_pages=max_pages
        )
        
        print(f"[Naver Cafe Crawl] ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ: {len(posts_list)}ê°œ", file=sys.stderr)
        
        # ìƒì„¸ ìˆ˜ì§‘ (ìŠ¤ë§ˆíŠ¸ ì¬ê°œ: ê¸°ì¡´ post_idëŠ” ìŠ¤í‚µ)
        all_posts = []
        for post_info in posts_list:
            post_id = post_info.get('post_id')
            
            # ìŠ¤ë§ˆíŠ¸ ì¬ê°œ: ì´ë¯¸ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì€ ìŠ¤í‚µ
            if post_id in existing_post_ids:
                print(f"[Naver Cafe Crawl] ìŠ¤í‚µ (ì´ë¯¸ ìˆ˜ì§‘ë¨): {post_id}", file=sys.stderr)
                continue
            
            url = post_info.get('url')
            if not url:
                continue
            
            # ìƒì„¸ ìˆ˜ì§‘
            detail = crawler.crawl_article_detail(url, post_id)
            if detail:
                # ëª©ë¡ ì •ë³´ì™€ ìƒì„¸ ì •ë³´ ë³‘í•©
                merged_post = {
                    **post_info,
                    'content': detail.get('content', ''),
                    'member_id': detail.get('member_id') or post_info.get('member_id'),
                    'nickname': detail.get('nickname') or post_info.get('nickname'),
                    'comments': detail.get('comments', []),
                    'viewCount': 0,  # í•„ìš”ì‹œ ì¶”ê°€
                    'commentCount': len(detail.get('comments', []))
                }
                all_posts.append(merged_post)
                existing_post_ids.add(post_id)
            
            # Rate limiting
            time.sleep(random.uniform(3, 7))
        
        return {
            "success": True,
            "posts": all_posts,
            "total": len(all_posts)
        }
        
    except Exception as e:
        import traceback
        print(f"[Naver Cafe Crawl] ì˜¤ë¥˜: {e}", file=sys.stderr)
        print(traceback.format_exc(), file=sys.stderr)
        return {
            "success": False,
            "error": str(e)
        }
    finally:
        # ë¸Œë¼ìš°ì € ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
        if use_browser and crawler:
            try:
                crawler.close()
            except Exception as e:
                print(f"[Naver Cafe Crawl] ë¸Œë¼ìš°ì € ì¢…ë£Œ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}", file=sys.stderr)

def crawl_community(args):
    """ì»¤ë®¤ë‹ˆí‹° ì´ìŠˆ í¬ë¡¤ë§ ë° ëŒ“ê¸€ ìƒì„±"""
    try:
        gemini_key = os.getenv('GEMINI_API_KEY')
        if not gemini_key:
            return {"success": False, "error": "Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
            
        crawler = CommunityCrawler()
        analyzer = GeminiAnalyzer(gemini_key)
        
        # limit íŒŒë¼ë¯¸í„° ë°›ê¸° (ê¸°ë³¸ê°’ 30)
        # argsëŠ” Namespace ê°ì²´ì´ë¯€ë¡œ getattr ì‚¬ìš©
        limit = int(getattr(args, 'limit', 30))
        
        # ë¦¬ì–¼í”½ ì£¼ìš” í”„ë¡œê·¸ë¨ í‚¤ì›Œë“œ
        program_keywords = [
            {"showId": "nasolo", "keywords": ["ë‚˜ëŠ”ì†”ë¡œ", "ë‚˜ì†”", "ë‚¨ê·œí™"]},
            {"showId": "choegang-yagu-2025", "keywords": ["ìµœê°•ì•¼êµ¬", "ëª¬ìŠ¤í„°ì¦ˆ", "ê¹€ì„±ê·¼"]},
            {"showId": "nasolsagye", "keywords": ["ë‚˜ì†”ì‚¬ê³„", "ì‚¬ë‘ì€ ê³„ì†ëœë‹¤"]},
            {"showId": "dolsingles6", "keywords": ["ëŒì‹±ê¸€ì¦ˆ", "ëŒì‹±"]},
            {"showId": "hwanseung4", "keywords": ["í™˜ìŠ¹ì—°ì• ", "í™˜ì—°"]},
            {"showId": "solojihuk5", "keywords": ["ì†”ë¡œì§€ì˜¥"]},
            {"showId": "culinary-class-wars2", "keywords": ["í‘ë°±ìš”ë¦¬ì‚¬", "ì•ˆì„±ì¬", "ë°±ì¢…ì›"]},
            {"showId": "goal-girls-8", "keywords": ["ê³¨ë•Œë…€", "ê³¨ ë•Œë¦¬ëŠ” ê·¸ë…€ë“¤"]}
        ]
        
        all_results = []
        # limitì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨í•˜ê¸° ìœ„í•œ í”Œë˜ê·¸
        reached_limit = False
        
        # ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ ëœë¤í•˜ê²Œ ëª‡ ê°œì˜ í”„ë¡œê·¸ë¨ë§Œ ì„ íƒí•˜ê±°ë‚˜ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
        for prog in program_keywords:
            if reached_limit:
                break
                
            show_id = prog["showId"]
            kws = prog["keywords"]
            
            # ê° í”„ë¡œê·¸ë¨ë³„ í‚¤ì›Œë“œë¡œ 10ëŒ€ ì»¤ë®¤ë‹ˆí‹° ëª¨ë‹ˆí„°ë§
            # limitì„ ì „ë‹¬í•˜ì—¬ í¬ë¡¤ë§ ê°œìˆ˜ ì œí•œ
            remaining_limit = limit - len(all_results)
            if remaining_limit <= 0:
                reached_limit = True
                break
                
            posts = crawler.get_hot_posts(show_id, kws, limit=remaining_limit)
            
            for post in posts:
                # limitì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                if len(all_results) >= limit:
                    reached_limit = True
                    break
                    
                # AI ëŒ“ê¸€ ìƒì„±
                comment = analyzer.generate_viral_comment(post.get('content', ''), post.get('title', ''))
                post['suggestedComment'] = comment
                all_results.append(post)
                
        # ì¡°íšŒìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        all_results.sort(key=lambda x: x.get('viewCount', 0), reverse=True)
        
        return {
            "success": True,
            "posts": all_results[:limit] # limitë§Œí¼ë§Œ ë°˜í™˜
        }
    except Exception as e:
        return {"success": False, "error": str(e)}

def main():
    # ëª¨ë“  ê²½ê³  ë©”ì‹œì§€ë¥¼ ë¬´ì‹œí•˜ì—¬ JSON ì¶œë ¥ë§Œ ê¹¨ë—í•˜ê²Œ ìœ ì§€
    import warnings
    warnings.filterwarnings("ignore")
    
    # ëª¨ë“  printë¥¼ stderrë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (JSON ì¶œë ¥ë§Œ stdoutì—)
    original_stdout = sys.stdout
    sys.stdout = sys.stderr
    
    try:
        parser = argparse.ArgumentParser(description='Realpick Marketing Bridge')
        parser.add_argument('--args-file', type=str, help='JSON file with arguments')
        parser.add_argument('command', type=str, nargs='?', help='Command to execute')
        
        # ê³µí†µ ì¸ì
        parser.add_argument('--keywords', type=str, help='Search keywords')
        parser.add_argument('--max-results', type=int, help='Maximum results')
        parser.add_argument('--video-id', type=str, help='YouTube video ID')
        parser.add_argument('--title', type=str, help='Video title')
        parser.add_argument('--desc', type=str, help='Video description')
        
        args, _ = parser.parse_known_args()
        
        # --args-fileì´ ì œê³µë˜ë©´ JSON íŒŒì¼ì—ì„œ ì¸ì ë¡œë“œ
        if args.args_file and os.path.exists(args.args_file):
            with open(args.args_file, 'r', encoding='utf-8') as f:
                json_args = json.load(f)
            
            # JSON ì¸ìë¥¼ argparse.Namespaceë¡œ ë³€í™˜
            for key, value in json_args.items():
                if key == 'command':
                    args.command = value
                else:
                    setattr(args, key.replace('-', '_'), value)
        
        # ëª…ë ¹ì–´ì— ë”°ë¼ í•¨ìˆ˜ ì‹¤í–‰
        result = None
        if args.command == 'crawl-youtube':
            result = crawl_youtube(args)
        elif args.command == 'analyze-video':
            result = analyze_video(args)
        elif args.command == 'crawl-community':
            result = crawl_community(args)
        elif args.command == 'crawl-naver-cafe':
            result = crawl_naver_cafe(args)
        else:
            result = {
                "success": False,
                "error": f"Unknown command: {args.command}"
            }
        
        # JSON ì¶œë ¥ (ë°˜ë“œì‹œ stdoutì— í•œ ì¤„ë¡œ)
        sys.stdout = original_stdout
        print(json.dumps(result, ensure_ascii=False))
        sys.stdout.flush()
        
    except Exception as e:
        import traceback
        # ì˜¤ë¥˜ë„ stdoutìœ¼ë¡œ JSON í˜•ì‹ìœ¼ë¡œ
        sys.stdout = original_stdout
        print(json.dumps({
            "success": False, 
            "error": str(e),
            "trace": traceback.format_exc()
        }, ensure_ascii=False))
        sys.stdout.flush()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        print(json.dumps({"success": False, "error": str(e)}, ensure_ascii=False))
        sys.stdout.flush()
        sys.exit(1)
